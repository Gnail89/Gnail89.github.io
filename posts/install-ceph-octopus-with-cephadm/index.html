<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Install Ceph Octopus With Cephadm | 📕W.'s Blog</title>
<meta name=keywords content="ceph,cephadm"><meta name=description content="1. 环境信息 CentOS 7 禁用firewalld 禁用selinux 时钟同步 设置主机名 ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. 安装依赖包 所有节点都需要安装
安装epel和elrepo软件源 yum install -y epel-release elrepo-release 安装python3 yum install -y python3 升级kernel至最新 yum --enablerepo=elrepo-kernel install -y kernel-ml # 设置默认使用的内核 grubby --default-index grubby --info=ALL grubby --set-default-index=0 安装docker-ce软件源 cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo 安装最新版本docker yum install -y docker-ce systemctl start docker systemctl enable docker 重启生效 3. 安装cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x ."><meta name=author content><link rel=canonical href=https://gnail89.github.io/posts/install-ceph-octopus-with-cephadm/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.5b9ae0304f93db6cc493f51846f012428af399c614b4f2fbdb7fa59dd4d5ef5b.js integrity="sha256-W5rgME+T22zEk/UYRvASQorzmcYUtPL723+lndTV71s=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://gnail89.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gnail89.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gnail89.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gnail89.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gnail89.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.120.3"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Install Ceph Octopus With Cephadm"><meta property="og:description" content="1. 环境信息 CentOS 7 禁用firewalld 禁用selinux 时钟同步 设置主机名 ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. 安装依赖包 所有节点都需要安装
安装epel和elrepo软件源 yum install -y epel-release elrepo-release 安装python3 yum install -y python3 升级kernel至最新 yum --enablerepo=elrepo-kernel install -y kernel-ml # 设置默认使用的内核 grubby --default-index grubby --info=ALL grubby --set-default-index=0 安装docker-ce软件源 cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo 安装最新版本docker yum install -y docker-ce systemctl start docker systemctl enable docker 重启生效 3. 安装cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x ."><meta property="og:type" content="article"><meta property="og:url" content="https://gnail89.github.io/posts/install-ceph-octopus-with-cephadm/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-22T17:24:19+08:00"><meta property="article:modified_time" content="2022-11-22T17:24:19+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Install Ceph Octopus With Cephadm"><meta name=twitter:description content="1. 环境信息 CentOS 7 禁用firewalld 禁用selinux 时钟同步 设置主机名 ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. 安装依赖包 所有节点都需要安装
安装epel和elrepo软件源 yum install -y epel-release elrepo-release 安装python3 yum install -y python3 升级kernel至最新 yum --enablerepo=elrepo-kernel install -y kernel-ml # 设置默认使用的内核 grubby --default-index grubby --info=ALL grubby --set-default-index=0 安装docker-ce软件源 cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo 安装最新版本docker yum install -y docker-ce systemctl start docker systemctl enable docker 重启生效 3. 安装cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x ."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://gnail89.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Install Ceph Octopus With Cephadm","item":"https://gnail89.github.io/posts/install-ceph-octopus-with-cephadm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Install Ceph Octopus With Cephadm","name":"Install Ceph Octopus With Cephadm","description":"1. 环境信息 CentOS 7 禁用firewalld 禁用selinux 时钟同步 设置主机名 ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. 安装依赖包 所有节点都需要安装\n安装epel和elrepo软件源 yum install -y epel-release elrepo-release 安装python3 yum install -y python3 升级kernel至最新 yum --enablerepo=elrepo-kernel install -y kernel-ml # 设置默认使用的内核 grubby --default-index grubby --info=ALL grubby --set-default-index=0 安装docker-ce软件源 cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo 安装最新版本docker yum install -y docker-ce systemctl start docker systemctl enable docker 重启生效 3. 安装cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x .","keywords":["ceph","cephadm"],"articleBody":"1. 环境信息 CentOS 7 禁用firewalld 禁用selinux 时钟同步 设置主机名 ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. 安装依赖包 所有节点都需要安装\n安装epel和elrepo软件源 yum install -y epel-release elrepo-release 安装python3 yum install -y python3 升级kernel至最新 yum --enablerepo=elrepo-kernel install -y kernel-ml # 设置默认使用的内核 grubby --default-index grubby --info=ALL grubby --set-default-index=0 安装docker-ce软件源 cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo 安装最新版本docker yum install -y docker-ce systemctl start docker systemctl enable docker 重启生效 3. 安装cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x ./cephadm ./cephadm add-repo --release octopus #添加repo软件源 ./cephadm install #升级最新cephadm包 4. 初始化ceph集群 初始化第一个mon节点 cephadm install ceph-common #安装ceph cli工具 #查看编排后端是cephadm，如果是显示rook这里后端就会显示rook。 ceph orch status mkdir -p /etc/ceph cephadm bootstrap --mon-ip 192.168.61.129 #初始化一个mon节点，获取最新的容器镜像到本地 # 获取mon容器shell cephadm shell --fsid ec45b570-5432-11ed-9ea9-000c29cb9f51 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring 引导完成一个单节点群集，程序会做如下事情：\n在本地主机上为新集群创建monitor 和 manager daemon守护程序。 为Ceph集群生成一个新的SSH密钥，并将其添加到root用户的/root/.ssh/authorized_keys文件中。 将与新群集进行通信所需的最小配置文件保存到/etc/ceph/ceph.conf。 向/etc/ceph/ceph.client.admin.keyring写入client.admin管理secret key的副本。 将public key的副本写入/etc/ceph/ceph.pub。 初始的mgr信息：\nURL: https://ceph-node01:8443 User: admin Password: 安装提示获取密码 (可选)如果忘记mgr密码可以通过以下方法重置密码(将密码写入password文件中，通过命令导入密码)\nceph dashboard ac-user-set-password admin -i password 当前节点安装后容器列表中的角色： mon.ceph-node01 # monitor节点 mgr.ceph-node01.hnlomt # mgr管理节点 prometheus.ceph-node01 # prometheus监控 alertmanager.ceph-node01 # prometheus告警相关 grafana.ceph-node01 # 监控告警展示 node-exporter.ceph-node01 # 数据采集node节点 crash.ceph-node01 # crash相关 分发cephadm节点公钥到其余节点： ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node02 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node03 新增ceph主机到集群新增ceph主机到集群 ceph orch host add ceph-node02 192.168.61.130 ceph orch host add ceph-node03 192.168.61.131 调整mon节点分布 默认mon节点是5个，随机分布在集群主机上，可以根据实际情况，指定部署ceph mon节点（orch模块自动管理）\nceph orch apply mon \"ceph-node01,ceph-node02,ceph-node03\" 指定部署mon数量，orch模块自动部署，默认值为5 ceph orch apply mon 3 指定部署mon到指定标签节点（orch模块自动管理） ceph orch apply mon label:mon 禁用mon自动部署（可选） 这种情况下需要手动部署mon节点，mon节点不受orch模块自动管理\nceph orch apply mon --unmanaged 手动指定节点部署mon（可选） ceph orch daemon add mon ceph-node02:192.168.61.130 ceph orch daemon add mon ceph-node03:192.168.61.131 对节点打标签(可选) ceph orch host label add ceph-node01 mon ceph orch host label add ceph-node02 mon ceph orch host label add ceph-node03 mon ceph orch host label add ceph-node01 _admin ceph orch host label add ceph-node01 mgr ceph orch host label add ceph-node02 mgr 查询集群状态 查看Service状态 ceph orch ls NAME RUNNING REFRESHED AGE PLACEMENT IMAGNAME IMAGE ID alertmanager 1/1 7m ago 23h count:1 quay.iprometheus/alertmanager:v0.20.0 0881eb8f169f crash 3/3 7m ago 23h * quay.io/cepceph:v15 93146564743f grafana 1/1 7m ago 23h count:1 quay.io/cepceph-grafana:6.7.4 557c83e11646 mgr 2/2 7m ago 23h count:2 quay.io/cepceph:v15 93146564743f mon 3/3 7m ago 118m ceph-node01;ceph-node02;ceph-node03 quay.io/cepceph:v15 93146564743f node-exporter 3/3 7m ago 23h * quay.iprometheus/node-exporter:v0.18.1 e5a616e4b9cf prometheus 1/1 7m ago 23h count:1 quay.iprometheus/prometheus:v2.18.1 de242295e225 查询Deamon状态 ceph orch ps NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ceph-node01 ceph-node01 running (2h) 3m ago 4w 0.20.0 quay.io/prometheus/alertmanager:v0.20.0 0881eb8f169f 689490756acc crash.ceph-node01 ceph-node01 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 5c05118fec32 crash.ceph-node02 ceph-node02 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f bbc408935136 crash.ceph-node03 ceph-node03 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 87c1e705053e grafana.ceph-node01 ceph-node01 running (2h) 3m ago 4w 6.7.4 quay.io/ceph/ceph-grafana:6.7.4 557c83e11646 e211ea6be042 mgr.ceph-node01.wmeddo ceph-node01 running (2h) 3m ago 5d 15.2.17 quay.io/ceph/ceph:v15 93146564743f e465de9c3e9d mgr.ceph-node02.hyipxw ceph-node02 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 5026b159b240 mon.ceph-node01 ceph-node01 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 3f48f4045628 mon.ceph-node02 ceph-node02 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 3cf45764cafb mon.ceph-node03 ceph-node03 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f f689c97923f6 node-exporter.ceph-node01 ceph-node01 running (2h) 3m ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 12aec21e4c45 node-exporter.ceph-node02 ceph-node02 running (2h) 3m ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 4fcfb820aa3b node-exporter.ceph-node03 ceph-node03 running (2h) 3m ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 8aee37a4868b prometheus.ceph-node01 ceph-node01 running (2h) 3m ago 4w 2.18.1 quay.io/prometheus/prometheus:v2.18.1 de242295e225 aa30615a1ce0 查看orch管理的主机列表 ceph orch host ls HOST ADDR LABELS STATUS ceph-node01 ceph-node01 _admin mgr mon ceph-node02 ceph-node02 mgr mon ceph-node03 ceph-node03 mon 5. 部署osd 添加OSD需求满足以下所有条件：\n设备必须没有分区。\n设备不得具有任何LVM状态。\n不得安装设备。\n该设备不得包含文件系统。\n该设备不得包含Ceph BlueStore OSD。\n设备必须大于5 GB。\norch模块自动部署osd ceph orch apply osd --all-available-devices 暂停orch模块自动管理osd（可选） ceph orch apply osd --all-available-devices --unmanaged=true 手动新增osd（可选） ceph orch device ls ceph orch device zap ceph-node01 /dev/sdb --force ceph orch daemon add osd ceph-node01:/dev/sdb 移除osd ceph orch daemon stop osd.2 ceph orch osd rm 2 #等待数据平衡 ceph orch osd rm status ceph osd tree ceph osd out osd.2 #等待数据平衡 ceph osd purge osd.2 --force ceph osd crush ls ceph-node01 ceph orch daemon rm osd.2 --force 问题：wipefs: error: /dev/sdb: probing initialization failed: Device or resource busy 解决办法：\n移除逻辑卷： dmsetup remove --force ceph--cd03e720--2f3e--497e--8f7a--97432c2b675d-osd--block--c4b4700e--1001--473a--ab05--470b7e9c711e 擦除文件系统的签名 wipefs -a /dev/sdb 重新初始化 ceph orch device zap ceph-node01 /dev/sdb --force 问题：集群所有mgr全部down，集群报warning，无法使用orch管理。 找到mgr一个节点，mgr一般保存位置为：/var/lib/ceph/{fsid}/mgr.node.name\n通过cephadm临时拉起mgr服务：\ncephadm run --name {mgr.node.name} --fsid {fsid} 然后用orch模块拉起剩余mgr服务： ceph orch ps ceph orch daemon start mgr.ceph-node01.name 停掉cephadm临时拉起的mgr，然后用orch模块重启所有mgr： ceph orch restart mgr 6. 部署CEPHFS服务 使用ceph fs创建 创建cephfs文件系统，新版本ceph会自动创建mds服务，使用cephfs文件系统需要一个或多个MDS服务。\nceph fs volume create cephfs --placement=3 ceph fs status # 或者手动部署mds守护进程： ceph orch apply mds cephfs --placement 3 ceph orch ls 查看ceph fs状态，可以显示主备mds节点信息，文件系统情况\nceph fs status ceph fs ls ceph mds stat 手动创建 # 创建fs data池，启用erasure纠删功能，也可以使用replicated功能。 ceph osd pool create cephfs.cephfs.data 32 erasure # 创建fs metadata池，只能默认replicated复制功能 ceph osd pool create cephfs.cephfs.meta 32 # 创建ceph fs，附上metadata和data池 ceph fs new cephfs cephfs.cephfs.meta cephfs.cephfs.data # 手动部署cephfs mds服务 ceph orch apply mds cephfs --placement=3 （可选）可将cephfs的数据池设置启用纠删码(pool创建时需要指定为erasure参数)，然后启用ec overwrites。但cephfs metadata元数据池不能使用纠删码，因为元数据使用OMAP数据结构所以不支持。\nceph osd pool set cephfs.cephfs.data allow_ec_overwrites true Linux挂载cephfs文件系统方法 生成ceph配置文件至客户端主机 mkdir -p -m 755 /etc/ceph ssh {user}@{mon-host} \"sudo ceph config generate-minimal-conf\" | sudo tee /etc/ceph/ceph.conf chmod 644 /etc/ceph/ceph.conf 生成cephx认证信息 ssh {user}@{mon-host} \"sudo ceph fs authorize cephfs client.mountfs / rw\" | sudo tee /etceph/ceph.client.mountfs.keyring chmod 600 /etc/ceph/ceph.client.mountfs.keyring 当使用内核驱动挂载cephfs时使用（泄露密钥风险） # mount -t ceph {device-string}:{path-to-mounted} {mount-point} -o {key-value-args{other-args} mount -t ceph 192.168.61.129:6789,192.168.61.130:6789,192.168.61.131:6789:/ /mnt -name=mountfs,secret=AQBtO3dj8bjLNxAAGFHw02lPnlbapyAVwwIBuA== 使用mount.ceph挂载（需要安装ceph-common包） mount -t ceph :/ /mnt -o name=mountfs,fs=cephfs 写入/etc/fstab文件 :/ /mnt ceph name=mountfs,[fs=cephfs,secret=str,]noatime,[nodiratime,]_netdev 0 2 7. 部署CEPH NFS服务 仅支持 NFSv4 协议。\n创建新池 ceph osd pool create ganesha_data 32 ceph osd pool application enable ganesha_data nfs 创建NFS网关 ceph orch apply nfs nfs ganesha_data --placement=2 查看Service状态 ceph orch ls NAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID alertmanager 1/1 46s ago 4w count:1 quay.io/prometheus/alertmanager:v0.20.0 0881eb8f169f crash 3/3 48s ago 4w * quay.io/ceph/ceph:v15 93146564743f grafana 1/1 46s ago 4w count:1 quay.io/ceph/ceph-grafana:6.7.4 557c83e11646 mds.cephfs 2/2 48s ago 5d count:2 quay.io/ceph/ceph:v15 93146564743f mgr 2/2 48s ago 9d ceph-node01;ceph-node02 quay.io/ceph/ceph:v15 93146564743f mon 3/3 48s ago 4w ceph-node01;ceph-node02;ceph-node03 quay.io/ceph/ceph:v15 93146564743f nfs.nfs 2/2 46s ago 11m count:2 quay.io/ceph/ceph:v15 93146564743f node-exporter 3/3 48s ago 4w * quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf osd.all-available-devices 3/3 48s ago 9d * quay.io/ceph/ceph:v15 93146564743f prometheus 1/1 46s ago 4w count:1 quay.io/prometheus/prometheus:v2.18.1 de242295e225 查看daemon状态 ceph orch ps NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ceph-node01 ceph-node01 running (5h) 81s ago 4w 0.20.0 quay.io/prometheus/alertmanager:v0.20.0 0881eb8f169f 689490756acc crash.ceph-node01 ceph-node01 running (5h) 81s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 5c05118fec32 crash.ceph-node02 ceph-node02 running (5h) 84s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f bbc408935136 crash.ceph-node03 ceph-node03 running (5h) 58s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 87c1e705053e grafana.ceph-node01 ceph-node01 running (5h) 81s ago 4w 6.7.4 quay.io/ceph/ceph-grafana:6.7.4 557c83e11646 e211ea6be042 mds.cephfs.ceph-node02.mxqelr ceph-node02 running (5h) 84s ago 5d 15.2.17 quay.io/ceph/ceph:v15 93146564743f 549606028df4 mds.cephfs.ceph-node03.xhcgto ceph-node03 running (5h) 58s ago 5d 15.2.17 quay.io/ceph/ceph:v15 93146564743f ee51fb9c1a8b mgr.ceph-node01.wmeddo ceph-node01 running (5h) 81s ago 6d 15.2.17 quay.io/ceph/ceph:v15 93146564743f e465de9c3e9d mgr.ceph-node02.hyipxw ceph-node02 running (5h) 84s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 5026b159b240 mon.ceph-node01 ceph-node01 running (5h) 81s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 3f48f4045628 mon.ceph-node02 ceph-node02 running (5h) 84s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 3cf45764cafb mon.ceph-node03 ceph-node03 running (5h) 58s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f f689c97923f6 nfs.nfs.ceph-node01 ceph-node01 running (11m) 81s ago 11m 3.3 quay.io/ceph/ceph:v15 93146564743f 117e58e5990c nfs.nfs.ceph-node03 ceph-node03 running (60s) 58s ago 11m 3.3 quay.io/ceph/ceph:v15 93146564743f 3662ecedd542 node-exporter.ceph-node01 ceph-node01 running (5h) 81s ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 12aec21e4c45 node-exporter.ceph-node02 ceph-node02 running (5h) 84s ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 4fcfb820aa3b node-exporter.ceph-node03 ceph-node03 running (5h) 58s ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 8aee37a4868b osd.0 ceph-node03 running (5h) 58s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 4d8635a833cc osd.1 ceph-node02 running (5h) 84s ago 3w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 6fa84fe728c7 osd.2 ceph-node01 running (5h) 81s ago 9d 15.2.17 quay.io/ceph/ceph:v15 93146564743f 16d42b3583ea prometheus.ceph-node01 ceph-node01 running (5h) 81s ago 4w 2.18.1 quay.io/prometheus/prometheus:v2.18.1 de242295e225 aa30615a1ce0 NFS 高可用(高版本支持) ingress nfs 部署文件：ingress-nfs.yml service_type: ingress service_id: nfs.nfs placement: count: 2 spec: backend_service: nfs.nfs frontend_port: 2050 # 对外服务端口 monitor_port: 9000 # haproxy查看状态页面 virtual_ip: 192.168.61.140/24 部署 ceph orch apply -i ingress-nfs.yaml 8. 部署RGW服务 ceph orch apply rgw object-service us-east-1 --placement=3 默认情况下，cephadm会自动创建realm和zone，或者也可以手动创建realm，zonegroup，zone。\nradosgw-admin realm create --rgw-realm= --default radosgw-admin zonegroup create --rgw-zonegroup= --master --default radosgw-admin zone create --rgw-zonegroup= --rgw-zone= --master --default radosgw-admin period update --rgw-realm= --commit ","wordCount":"1244","inLanguage":"en","datePublished":"2022-11-22T17:24:19+08:00","dateModified":"2022-11-22T17:24:19+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gnail89.github.io/posts/install-ceph-octopus-with-cephadm/"},"publisher":{"@type":"Organization","name":"📕W.'s Blog","logo":{"@type":"ImageObject","url":"https://gnail89.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gnail89.github.io accesskey=h title="📕W.'s Blog (Alt + H)">📕W.'s Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://gnail89.github.io/search/ title=🔍Search><span>🔍Search</span></a></li><li><a href=https://gnail89.github.io/archives/ title=📚Archive><span>📚Archive</span></a></li><li><a href=https://gnail89.github.io/tags/ title=🔖Tags><span>🔖Tags</span></a></li><li><a href=https://gnail89.github.io/categories/ title=🧩Categories><span>🧩Categories</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gnail89.github.io>Home</a>&nbsp;»&nbsp;<a href=https://gnail89.github.io/posts/>Posts</a></div><h1 class=post-title>Install Ceph Octopus With Cephadm</h1><div class=post-meta>&lt;span title='2022-11-22 17:24:19 +0800 +0800'>November 22, 2022&lt;/span>&amp;nbsp;·&amp;nbsp;6 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-%e7%8e%af%e5%a2%83%e4%bf%a1%e6%81%af aria-label="1. 环境信息">1. 环境信息</a></li><li><a href=#2-%e5%ae%89%e8%a3%85%e4%be%9d%e8%b5%96%e5%8c%85 aria-label="2. 安装依赖包">2. 安装依赖包</a></li><li><a href=#3-%e5%ae%89%e8%a3%85cephadm aria-label="3. 安装cephadm">3. 安装cephadm</a></li><li><a href=#4-%e5%88%9d%e5%a7%8b%e5%8c%96ceph%e9%9b%86%e7%be%a4 aria-label="4. 初始化ceph集群">4. 初始化ceph集群</a><ul><li><a href=#%e5%88%9d%e5%a7%8b%e5%8c%96%e7%ac%ac%e4%b8%80%e4%b8%aamon%e8%8a%82%e7%82%b9 aria-label=初始化第一个mon节点>初始化第一个mon节点</a></li><li><a href=#%e6%9f%a5%e8%af%a2%e9%9b%86%e7%be%a4%e7%8a%b6%e6%80%81 aria-label=查询集群状态>查询集群状态</a></li></ul></li><li><a href=#5-%e9%83%a8%e7%bd%b2osd aria-label="5. 部署osd">5. 部署osd</a><ul><li><a href=#%e9%97%ae%e9%a2%98wipefs-error-devsdb-probing-initialization-failed-device-or-resource-busy aria-label="问题：wipefs: error: /dev/sdb: probing initialization failed: Device or resource busy">问题：wipefs: error: /dev/sdb: probing initialization failed: Device or resource busy</a></li><li><a href=#%e9%97%ae%e9%a2%98%e9%9b%86%e7%be%a4%e6%89%80%e6%9c%89mgr%e5%85%a8%e9%83%a8down%e9%9b%86%e7%be%a4%e6%8a%a5warning%e6%97%a0%e6%b3%95%e4%bd%bf%e7%94%a8orch%e7%ae%a1%e7%90%86 aria-label=问题：集群所有mgr全部down，集群报warning，无法使用orch管理。>问题：集群所有mgr全部down，集群报warning，无法使用orch管理。</a></li></ul></li><li><a href=#6-%e9%83%a8%e7%bd%b2cephfs%e6%9c%8d%e5%8a%a1 aria-label="6. 部署CEPHFS服务">6. 部署CEPHFS服务</a><ul><li><a href=#%e4%bd%bf%e7%94%a8ceph-fs%e5%88%9b%e5%bb%ba aria-label="使用ceph fs创建">使用ceph fs创建</a></li><li><a href=#%e6%89%8b%e5%8a%a8%e5%88%9b%e5%bb%ba aria-label=手动创建>手动创建</a></li><li><a href=#linux%e6%8c%82%e8%bd%bdcephfs%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e6%96%b9%e6%b3%95 aria-label=Linux挂载cephfs文件系统方法>Linux挂载cephfs文件系统方法</a></li></ul></li><li><a href=#7-%e9%83%a8%e7%bd%b2ceph-nfs%e6%9c%8d%e5%8a%a1 aria-label="7. 部署CEPH NFS服务">7. 部署CEPH NFS服务</a><ul><li><a href=#%e5%88%9b%e5%bb%ba%e6%96%b0%e6%b1%a0 aria-label=创建新池>创建新池</a></li><li><a href=#%e5%88%9b%e5%bb%banfs%e7%bd%91%e5%85%b3 aria-label=创建NFS网关>创建NFS网关</a></li><li><a href=#nfs-%e9%ab%98%e5%8f%af%e7%94%a8%e9%ab%98%e7%89%88%e6%9c%ac%e6%94%af%e6%8c%81 aria-label="NFS 高可用(高版本支持)">NFS 高可用(高版本支持)</a></li></ul></li><li><a href=#8-%e9%83%a8%e7%bd%b2rgw%e6%9c%8d%e5%8a%a1 aria-label="8. 部署RGW服务">8. 部署RGW服务</a></li></ul></div></details></div><div class=post-content><h1 id=1-环境信息>1. 环境信息<a hidden class=anchor aria-hidden=true href=#1-环境信息>#</a></h1><ul><li>CentOS 7</li><li>禁用firewalld</li><li>禁用selinux</li><li>时钟同步</li><li>设置主机名</li></ul><table><thead><tr><th>ip addr</th><th>hostname</th></tr></thead><tbody><tr><td>192.168.61.129</td><td>ceph-node01</td></tr><tr><td>192.168.61.130</td><td>ceph-node02</td></tr><tr><td>192.168.61.131</td><td>ceph-node03</td></tr></tbody></table><h1 id=2-安装依赖包>2. 安装依赖包<a hidden class=anchor aria-hidden=true href=#2-安装依赖包>#</a></h1><blockquote><p>所有节点都需要安装</p></blockquote><ul><li>安装epel和elrepo软件源</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yum install -y epel-release elrepo-release
</span></span></code></pre></div><ul><li>安装python3</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yum install -y python3
</span></span></code></pre></div><ul><li>升级kernel至最新</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yum --enablerepo<span class=o>=</span>elrepo-kernel install -y kernel-ml
</span></span><span class=line><span class=cl><span class=c1># 设置默认使用的内核</span>
</span></span><span class=line><span class=cl>grubby --default-index
</span></span><span class=line><span class=cl>grubby --info<span class=o>=</span>ALL
</span></span><span class=line><span class=cl>grubby --set-default-index<span class=o>=</span><span class=m>0</span>
</span></span></code></pre></div><ul><li>安装docker-ce软件源</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>cd</span> /etc/yum.repos.d/
</span></span><span class=line><span class=cl>curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo
</span></span></code></pre></div><ul><li>安装最新版本docker</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yum install -y docker-ce
</span></span><span class=line><span class=cl>systemctl start docker
</span></span><span class=line><span class=cl>systemctl <span class=nb>enable</span> docker
</span></span></code></pre></div><ul><li>重启生效</li></ul><h1 id=3-安装cephadm>3. 安装cephadm<a hidden class=anchor aria-hidden=true href=#3-安装cephadm>#</a></h1><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm
</span></span><span class=line><span class=cl>chmod +x ./cephadm
</span></span><span class=line><span class=cl>./cephadm add-repo --release octopus        <span class=c1>#添加repo软件源</span>
</span></span><span class=line><span class=cl>./cephadm install    <span class=c1>#升级最新cephadm包</span>
</span></span></code></pre></div><h1 id=4-初始化ceph集群>4. 初始化ceph集群<a hidden class=anchor aria-hidden=true href=#4-初始化ceph集群>#</a></h1><h2 id=初始化第一个mon节点>初始化第一个mon节点<a hidden class=anchor aria-hidden=true href=#初始化第一个mon节点>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cephadm install ceph-common     <span class=c1>#安装ceph cli工具</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#查看编排后端是cephadm，如果是显示rook这里后端就会显示rook。</span>
</span></span><span class=line><span class=cl>ceph orch status
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>mkdir -p /etc/ceph
</span></span><span class=line><span class=cl>cephadm bootstrap --mon-ip 192.168.61.129       <span class=c1>#初始化一个mon节点，获取最新的容器镜像到本地</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 获取mon容器shell</span>
</span></span><span class=line><span class=cl>cephadm shell --fsid ec45b570-5432-11ed-9ea9-000c29cb9f51 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring
</span></span></code></pre></div><p>引导完成一个单节点群集，程序会做如下事情：</p><ul><li>在本地主机上为新集群创建monitor 和 manager daemon守护程序。</li><li>为Ceph集群生成一个新的SSH密钥，并将其添加到root用户的/root/.ssh/authorized_keys文件中。</li><li>将与新群集进行通信所需的最小配置文件保存到/etc/ceph/ceph.conf。</li><li>向/etc/ceph/ceph.client.admin.keyring写入client.admin管理secret key的副本。</li><li>将public key的副本写入/etc/ceph/ceph.pub。</li></ul><blockquote><p>初始的mgr信息：</p></blockquote><pre tabindex=0><code>URL: https://ceph-node01:8443
User: admin
Password: 安装提示获取密码
</code></pre><p>(可选)如果忘记mgr密码可以通过以下方法重置密码(将密码写入password文件中，通过命令导入密码)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph dashboard ac-user-set-password admin -i password
</span></span></code></pre></div><ul><li>当前节点安装后容器列表中的角色：</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>mon.ceph-node01             <span class=c1># monitor节点</span>
</span></span><span class=line><span class=cl>mgr.ceph-node01.hnlomt      <span class=c1># mgr管理节点</span>
</span></span><span class=line><span class=cl>prometheus.ceph-node01      <span class=c1># prometheus监控</span>
</span></span><span class=line><span class=cl>alertmanager.ceph-node01    <span class=c1># prometheus告警相关</span>
</span></span><span class=line><span class=cl>grafana.ceph-node01         <span class=c1># 监控告警展示</span>
</span></span><span class=line><span class=cl>node-exporter.ceph-node01   <span class=c1># 数据采集node节点</span>
</span></span><span class=line><span class=cl>crash.ceph-node01           <span class=c1># crash相关</span>
</span></span></code></pre></div><ul><li>分发cephadm节点公钥到其余节点：</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node02
</span></span><span class=line><span class=cl>ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node03
</span></span></code></pre></div><ul><li>新增ceph主机到集群新增ceph主机到集群</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch host add ceph-node02 192.168.61.130
</span></span><span class=line><span class=cl>ceph orch host add ceph-node03 192.168.61.131
</span></span></code></pre></div><ul><li>调整mon节点分布</li></ul><p>默认mon节点是5个，随机分布在集群主机上，可以根据实际情况，指定部署ceph mon节点（orch模块自动管理）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply mon <span class=s2>&#34;ceph-node01,ceph-node02,ceph-node03&#34;</span>
</span></span></code></pre></div><ul><li>指定部署mon数量，orch模块自动部署，默认值为5</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply mon <span class=m>3</span>
</span></span></code></pre></div><ul><li>指定部署mon到指定标签节点（orch模块自动管理）</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply mon label:mon
</span></span></code></pre></div><ul><li>禁用mon自动部署（可选）</li></ul><p>这种情况下需要手动部署mon节点，mon节点不受orch模块自动管理</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply mon --unmanaged
</span></span></code></pre></div><ul><li>手动指定节点部署mon（可选）</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch daemon add mon ceph-node02:192.168.61.130
</span></span><span class=line><span class=cl>ceph orch daemon add mon ceph-node03:192.168.61.131
</span></span></code></pre></div><ul><li>对节点打标签(可选)</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch host label add ceph-node01 mon
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node02 mon
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node03 mon
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node01 _admin
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node01 mgr
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node02 mgr
</span></span></code></pre></div><h2 id=查询集群状态>查询集群状态<a hidden class=anchor aria-hidden=true href=#查询集群状态>#</a></h2><ul><li>查看Service状态</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ls
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>NAME           RUNNING  REFRESHED  AGE   PLACEMENT                            IMAGNAME                                IMAGE ID      
</span></span><span class=line><span class=cl>alertmanager       1/1  7m ago     23h   count:1                              quay.iprometheus/alertmanager:v0.20.0   0881eb8f169f  
</span></span><span class=line><span class=cl>crash              3/3  7m ago     23h   *                                    quay.io/cepceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>grafana            1/1  7m ago     23h   count:1                              quay.io/cepceph-grafana:6.7.4           557c83e11646  
</span></span><span class=line><span class=cl>mgr                2/2  7m ago     23h   count:2                              quay.io/cepceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>mon                3/3  7m ago     118m  ceph-node01<span class=p>;</span>ceph-node02<span class=p>;</span>ceph-node03  quay.io/cepceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>node-exporter      3/3  7m ago     23h   *                                    quay.iprometheus/node-exporter:v0.18.1  e5a616e4b9cf  
</span></span><span class=line><span class=cl>prometheus         1/1  7m ago     23h   count:1                              quay.iprometheus/prometheus:v2.18.1     de242295e225  
</span></span></code></pre></div><ul><li>查询Deamon状态</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ps
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                           HOST         STATUS        REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID  
</span></span><span class=line><span class=cl>alertmanager.ceph-node01       ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   0.20.0   quay.io/prometheus/alertmanager:v0.20.0   0881eb8f169f  689490756acc  
</span></span><span class=line><span class=cl>crash.ceph-node01              ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  5c05118fec32  
</span></span><span class=line><span class=cl>crash.ceph-node02              ceph-node02  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  bbc408935136  
</span></span><span class=line><span class=cl>crash.ceph-node03              ceph-node03  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  87c1e705053e  
</span></span><span class=line><span class=cl>grafana.ceph-node01            ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   6.7.4    quay.io/ceph/ceph-grafana:6.7.4           557c83e11646  e211ea6be042  
</span></span><span class=line><span class=cl>mgr.ceph-node01.wmeddo         ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     5d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  e465de9c3e9d  
</span></span><span class=line><span class=cl>mgr.ceph-node02.hyipxw         ceph-node02  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  5026b159b240  
</span></span><span class=line><span class=cl>mon.ceph-node01                ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  3f48f4045628  
</span></span><span class=line><span class=cl>mon.ceph-node02                ceph-node02  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  3cf45764cafb  
</span></span><span class=line><span class=cl>mon.ceph-node03                ceph-node03  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  f689c97923f6  
</span></span><span class=line><span class=cl>node-exporter.ceph-node01      ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  12aec21e4c45  
</span></span><span class=line><span class=cl>node-exporter.ceph-node02      ceph-node02  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  4fcfb820aa3b  
</span></span><span class=line><span class=cl>node-exporter.ceph-node03      ceph-node03  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  8aee37a4868b  
</span></span><span class=line><span class=cl>prometheus.ceph-node01         ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   2.18.1   quay.io/prometheus/prometheus:v2.18.1     de242295e225  aa30615a1ce0  
</span></span></code></pre></div><ul><li>查看orch管理的主机列表</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch host ls
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>HOST         ADDR         LABELS          STATUS  
</span></span><span class=line><span class=cl>ceph-node01  ceph-node01  _admin mgr mon          
</span></span><span class=line><span class=cl>ceph-node02  ceph-node02  mgr mon                 
</span></span><span class=line><span class=cl>ceph-node03  ceph-node03  mon 
</span></span></code></pre></div><h1 id=5-部署osd>5. 部署osd<a hidden class=anchor aria-hidden=true href=#5-部署osd>#</a></h1><p>添加OSD需求满足以下所有条件：</p><blockquote><p>设备必须没有分区。<br>设备不得具有任何LVM状态。<br>不得安装设备。<br>该设备不得包含文件系统。<br>该设备不得包含Ceph BlueStore OSD。<br>设备必须大于5 GB。</p></blockquote><ul><li>orch模块自动部署osd</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply osd --all-available-devices
</span></span></code></pre></div><ul><li>暂停orch模块自动管理osd（可选）</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply osd --all-available-devices --unmanaged<span class=o>=</span><span class=nb>true</span>
</span></span></code></pre></div><ul><li>手动新增osd（可选）</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch device ls
</span></span><span class=line><span class=cl>ceph orch device zap ceph-node01 /dev/sdb --force
</span></span><span class=line><span class=cl>ceph orch daemon add osd ceph-node01:/dev/sdb
</span></span></code></pre></div><ul><li>移除osd</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch daemon stop osd.2
</span></span><span class=line><span class=cl>ceph orch osd rm <span class=m>2</span>  <span class=c1>#等待数据平衡</span>
</span></span><span class=line><span class=cl>ceph orch osd rm status
</span></span><span class=line><span class=cl>ceph osd tree
</span></span><span class=line><span class=cl>ceph osd out osd.2      <span class=c1>#等待数据平衡</span>
</span></span><span class=line><span class=cl>ceph osd purge osd.2 --force
</span></span><span class=line><span class=cl>ceph osd crush ls ceph-node01
</span></span><span class=line><span class=cl>ceph orch daemon rm osd.2 --force
</span></span></code></pre></div><h2 id=问题wipefs-error-devsdb-probing-initialization-failed-device-or-resource-busy>问题：wipefs: error: /dev/sdb: probing initialization failed: Device or resource busy<a hidden class=anchor aria-hidden=true href=#问题wipefs-error-devsdb-probing-initialization-failed-device-or-resource-busy>#</a></h2><p>解决办法：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>移除逻辑卷：
</span></span><span class=line><span class=cl>dmsetup remove --force ceph--cd03e720--2f3e--497e--8f7a--97432c2b675d-osd--block--c4b4700e--1001--473a--ab05--470b7e9c711e
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>擦除文件系统的签名
</span></span><span class=line><span class=cl>wipefs -a /dev/sdb
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>重新初始化
</span></span><span class=line><span class=cl>ceph orch device zap ceph-node01 /dev/sdb --force
</span></span></code></pre></div><h2 id=问题集群所有mgr全部down集群报warning无法使用orch管理>问题：集群所有mgr全部down，集群报warning，无法使用orch管理。<a hidden class=anchor aria-hidden=true href=#问题集群所有mgr全部down集群报warning无法使用orch管理>#</a></h2><ul><li><p>找到mgr一个节点，mgr一般保存位置为：<code>/var/lib/ceph/{fsid}/mgr.node.name</code></p></li><li><p>通过cephadm临时拉起mgr服务：</p></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cephadm run --name <span class=o>{</span>mgr.node.name<span class=o>}</span> --fsid <span class=o>{</span>fsid<span class=o>}</span>
</span></span></code></pre></div><ul><li>然后用orch模块拉起剩余mgr服务：</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ps
</span></span><span class=line><span class=cl>ceph orch daemon start mgr.ceph-node01.name
</span></span></code></pre></div><ul><li>停掉cephadm临时拉起的mgr，然后用orch模块重启所有mgr：</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch restart mgr
</span></span></code></pre></div><h1 id=6-部署cephfs服务>6. 部署CEPHFS服务<a hidden class=anchor aria-hidden=true href=#6-部署cephfs服务>#</a></h1><h2 id=使用ceph-fs创建>使用ceph fs创建<a hidden class=anchor aria-hidden=true href=#使用ceph-fs创建>#</a></h2><p>创建cephfs文件系统，新版本ceph会自动创建mds服务，使用cephfs文件系统需要一个或多个MDS服务。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph fs volume create cephfs --placement<span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl>ceph fs status
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 或者手动部署mds守护进程：</span>
</span></span><span class=line><span class=cl>ceph orch apply mds cephfs --placement <span class=m>3</span>
</span></span><span class=line><span class=cl>ceph orch ls
</span></span></code></pre></div><p>查看ceph fs状态，可以显示主备mds节点信息，文件系统情况</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph fs status
</span></span><span class=line><span class=cl>ceph fs ls
</span></span><span class=line><span class=cl>ceph mds stat
</span></span></code></pre></div><h2 id=手动创建>手动创建<a hidden class=anchor aria-hidden=true href=#手动创建>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 创建fs data池，启用erasure纠删功能，也可以使用replicated功能。</span>
</span></span><span class=line><span class=cl>ceph osd pool create cephfs.cephfs.data <span class=m>32</span> erasure
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建fs metadata池，只能默认replicated复制功能</span>
</span></span><span class=line><span class=cl>ceph osd pool create cephfs.cephfs.meta <span class=m>32</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建ceph fs，附上metadata和data池</span>
</span></span><span class=line><span class=cl>ceph fs new cephfs cephfs.cephfs.meta cephfs.cephfs.data
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 手动部署cephfs mds服务</span>
</span></span><span class=line><span class=cl>ceph orch apply mds cephfs --placement<span class=o>=</span><span class=m>3</span>
</span></span></code></pre></div><p>（可选）可将cephfs的数据池设置启用纠删码(pool创建时需要指定为erasure参数)，然后启用ec overwrites。但cephfs metadata元数据池不能使用纠删码，因为元数据使用OMAP数据结构所以不支持。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph osd pool <span class=nb>set</span> cephfs.cephfs.data allow_ec_overwrites <span class=nb>true</span>
</span></span></code></pre></div><h2 id=linux挂载cephfs文件系统方法>Linux挂载cephfs文件系统方法<a hidden class=anchor aria-hidden=true href=#linux挂载cephfs文件系统方法>#</a></h2><ul><li>生成ceph配置文件至客户端主机</li></ul><pre tabindex=0><code>mkdir -p -m 755 /etc/ceph
ssh {user}@{mon-host} &#34;sudo ceph config generate-minimal-conf&#34; | sudo tee /etc/ceph/ceph.conf
chmod 644 /etc/ceph/ceph.conf
</code></pre><ul><li>生成cephx认证信息</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh <span class=o>{</span>user<span class=o>}</span>@<span class=o>{</span>mon-host<span class=o>}</span> <span class=s2>&#34;sudo ceph fs authorize cephfs client.mountfs / rw&#34;</span> <span class=p>|</span> sudo tee /etceph/ceph.client.mountfs.keyring
</span></span><span class=line><span class=cl>chmod <span class=m>600</span> /etc/ceph/ceph.client.mountfs.keyring
</span></span></code></pre></div><ul><li>当使用内核驱动挂载cephfs时使用（泄露密钥风险）</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># mount -t ceph {device-string}:{path-to-mounted} {mount-point} -o {key-value-args{other-args}</span>
</span></span><span class=line><span class=cl>mount -t ceph 192.168.61.129:6789,192.168.61.130:6789,192.168.61.131:6789:/ /mnt -name<span class=o>=</span>mountfs,secret<span class=o>=</span><span class=nv>AQBtO3dj8bjLNxAAGFHw02lPnlbapyAVwwIBuA</span><span class=o>==</span>
</span></span></code></pre></div><ul><li>使用mount.ceph挂载（需要安装ceph-common包）</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>mount -t ceph :/ /mnt -o <span class=nv>name</span><span class=o>=</span>mountfs,fs<span class=o>=</span>cephfs
</span></span></code></pre></div><ul><li>写入/etc/fstab文件</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>:/  /mnt  ceph  <span class=nv>name</span><span class=o>=</span>mountfs,<span class=o>[</span><span class=nv>fs</span><span class=o>=</span>cephfs,secret<span class=o>=</span>str,<span class=o>]</span>noatime,<span class=o>[</span>nodiratime,<span class=o>]</span>_netdev  <span class=m>0</span>  <span class=m>2</span>
</span></span></code></pre></div><h1 id=7-部署ceph-nfs服务>7. 部署CEPH NFS服务<a hidden class=anchor aria-hidden=true href=#7-部署ceph-nfs服务>#</a></h1><blockquote><p>仅支持 NFSv4 协议。</p></blockquote><h2 id=创建新池>创建新池<a hidden class=anchor aria-hidden=true href=#创建新池>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph osd pool create ganesha_data <span class=m>32</span>
</span></span><span class=line><span class=cl>ceph osd pool application <span class=nb>enable</span> ganesha_data nfs
</span></span></code></pre></div><h2 id=创建nfs网关>创建NFS网关<a hidden class=anchor aria-hidden=true href=#创建nfs网关>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply nfs nfs ganesha_data --placement<span class=o>=</span><span class=m>2</span> 
</span></span></code></pre></div><ul><li>查看Service状态</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ls
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                       RUNNING  REFRESHED  AGE  PLACEMENT                            IMAGE NAME                                IMAGE ID      
</span></span><span class=line><span class=cl>alertmanager                   1/1  46s ago    4w   count:1                              quay.io/prometheus/alertmanager:v0.20.0   0881eb8f169f  
</span></span><span class=line><span class=cl>crash                          3/3  48s ago    4w   *                                    quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>grafana                        1/1  46s ago    4w   count:1                              quay.io/ceph/ceph-grafana:6.7.4           557c83e11646  
</span></span><span class=line><span class=cl>mds.cephfs                     2/2  48s ago    5d   count:2                              quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>mgr                            2/2  48s ago    9d   ceph-node01<span class=p>;</span>ceph-node02              quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>mon                            3/3  48s ago    4w   ceph-node01<span class=p>;</span>ceph-node02<span class=p>;</span>ceph-node03  quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>nfs.nfs                        2/2  46s ago    11m  count:2                              quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>node-exporter                  3/3  48s ago    4w   *                                    quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  
</span></span><span class=line><span class=cl>osd.all-available-devices      3/3  48s ago    9d   *                                    quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>prometheus                     1/1  46s ago    4w   count:1                              quay.io/prometheus/prometheus:v2.18.1     de242295e225  
</span></span></code></pre></div><ul><li>查看daemon状态</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ps
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                           HOST         STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID  
</span></span><span class=line><span class=cl>alertmanager.ceph-node01       ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   0.20.0   quay.io/prometheus/alertmanager:v0.20.0   0881eb8f169f  689490756acc  
</span></span><span class=line><span class=cl>crash.ceph-node01              ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  5c05118fec32  
</span></span><span class=line><span class=cl>crash.ceph-node02              ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  bbc408935136  
</span></span><span class=line><span class=cl>crash.ceph-node03              ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  87c1e705053e  
</span></span><span class=line><span class=cl>grafana.ceph-node01            ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   6.7.4    quay.io/ceph/ceph-grafana:6.7.4           557c83e11646  e211ea6be042  
</span></span><span class=line><span class=cl>mds.cephfs.ceph-node02.mxqelr  ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    5d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  549606028df4  
</span></span><span class=line><span class=cl>mds.cephfs.ceph-node03.xhcgto  ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    5d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  ee51fb9c1a8b  
</span></span><span class=line><span class=cl>mgr.ceph-node01.wmeddo         ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    6d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  e465de9c3e9d  
</span></span><span class=line><span class=cl>mgr.ceph-node02.hyipxw         ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  5026b159b240  
</span></span><span class=line><span class=cl>mon.ceph-node01                ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  3f48f4045628  
</span></span><span class=line><span class=cl>mon.ceph-node02                ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  3cf45764cafb  
</span></span><span class=line><span class=cl>mon.ceph-node03                ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  f689c97923f6  
</span></span><span class=line><span class=cl>nfs.nfs.ceph-node01            ceph-node01  running <span class=o>(</span>11m<span class=o>)</span>  81s ago    11m  3.3      quay.io/ceph/ceph:v15                     93146564743f  117e58e5990c  
</span></span><span class=line><span class=cl>nfs.nfs.ceph-node03            ceph-node03  running <span class=o>(</span>60s<span class=o>)</span>  58s ago    11m  3.3      quay.io/ceph/ceph:v15                     93146564743f  3662ecedd542  
</span></span><span class=line><span class=cl>node-exporter.ceph-node01      ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  12aec21e4c45  
</span></span><span class=line><span class=cl>node-exporter.ceph-node02      ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  4fcfb820aa3b  
</span></span><span class=line><span class=cl>node-exporter.ceph-node03      ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  8aee37a4868b  
</span></span><span class=line><span class=cl>osd.0                          ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  4d8635a833cc  
</span></span><span class=line><span class=cl>osd.1                          ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    3w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  6fa84fe728c7  
</span></span><span class=line><span class=cl>osd.2                          ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    9d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  16d42b3583ea  
</span></span><span class=line><span class=cl>prometheus.ceph-node01         ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   2.18.1   quay.io/prometheus/prometheus:v2.18.1     de242295e225  aa30615a1ce0  
</span></span></code></pre></div><h2 id=nfs-高可用高版本支持>NFS 高可用(高版本支持)<a hidden class=anchor aria-hidden=true href=#nfs-高可用高版本支持>#</a></h2><ul><li>ingress nfs 部署文件：<code>ingress-nfs.yml</code></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>service_type</span><span class=p>:</span><span class=w> </span><span class=l>ingress</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>service_id</span><span class=p>:</span><span class=w> </span><span class=l>nfs.nfs</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>placement</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>count</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>backend_service</span><span class=p>:</span><span class=w> </span><span class=l>nfs.nfs</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>frontend_port</span><span class=p>:</span><span class=w> </span><span class=m>2050</span><span class=w>   </span><span class=c># 对外服务端口</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>monitor_port</span><span class=p>:</span><span class=w> </span><span class=m>9000</span><span class=w>    </span><span class=c># haproxy查看状态页面</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>virtual_ip</span><span class=p>:</span><span class=w> </span><span class=m>192.168.61.140</span><span class=l>/24</span><span class=w>
</span></span></span></code></pre></div><ul><li>部署</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply -i ingress-nfs.yaml
</span></span></code></pre></div><h1 id=8-部署rgw服务>8. 部署RGW服务<a hidden class=anchor aria-hidden=true href=#8-部署rgw服务>#</a></h1><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply rgw object-service us-east-1 --placement<span class=o>=</span><span class=m>3</span>
</span></span></code></pre></div><p>默认情况下，cephadm会自动创建realm和zone，或者也可以手动创建realm，zonegroup，zone。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>radosgw-admin realm create --rgw-realm<span class=o>=</span>&lt;realm-name&gt; --default
</span></span><span class=line><span class=cl>radosgw-admin zonegroup create --rgw-zonegroup<span class=o>=</span>&lt;zonegroup-name&gt;  --master --default
</span></span><span class=line><span class=cl>radosgw-admin zone create --rgw-zonegroup<span class=o>=</span>&lt;zonegroup-name&gt; --rgw-zone<span class=o>=</span>&lt;zone-name&gt; --master --default
</span></span><span class=line><span class=cl>radosgw-admin period update --rgw-realm<span class=o>=</span>&lt;realm-name&gt; --commit
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://gnail89.github.io/tags/ceph/>ceph</a></li><li><a href=https://gnail89.github.io/tags/cephadm/>cephadm</a></li></ul><nav class=paginav><a class=prev href=https://gnail89.github.io/posts/ssh-security/><span class=title>« Prev Page</span><br><span>SSH Security</span>
</a><a class=next href=https://gnail89.github.io/posts/markdown/><span class=title>Next Page »</span><br><span>Markdown基本语法</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://gnail89.github.io>📕W.'s Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>