<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Install Ceph Octopus With Cephadm | ğŸ“•W.'s Blog</title>
<meta name=keywords content="ceph,cephadm"><meta name=description content="1. ç¯å¢ƒä¿¡æ¯ CentOS 7 ç¦ç”¨firewalld ç¦ç”¨selinux æ—¶é’ŸåŒæ­¥ è®¾ç½®ä¸»æœºå ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. å®‰è£…ä¾èµ–åŒ… æ‰€æœ‰èŠ‚ç‚¹éƒ½éœ€è¦å®‰è£…
å®‰è£…epelå’Œelrepoè½¯ä»¶æº yum install -y epel-release elrepo-release å®‰è£…python3 yum install -y python3 å‡çº§kernelè‡³æœ€æ–° yum --enablerepo=elrepo-kernel install -y kernel-ml # è®¾ç½®é»˜è®¤ä½¿ç”¨çš„å†…æ ¸ grubby --default-index grubby --info=ALL grubby --set-default-index=0 å®‰è£…docker-ceè½¯ä»¶æº cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo å®‰è£…æœ€æ–°ç‰ˆæœ¬docker yum install -y docker-ce systemctl start docker systemctl enable docker é‡å¯ç”Ÿæ•ˆ 3. å®‰è£…cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x ."><meta name=author content><link rel=canonical href=https://gnail89.github.io/posts/install-ceph-octopus-with-cephadm/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.5b9ae0304f93db6cc493f51846f012428af399c614b4f2fbdb7fa59dd4d5ef5b.js integrity="sha256-W5rgME+T22zEk/UYRvASQorzmcYUtPL723+lndTV71s=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://gnail89.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gnail89.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gnail89.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gnail89.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gnail89.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.120.3"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Install Ceph Octopus With Cephadm"><meta property="og:description" content="1. ç¯å¢ƒä¿¡æ¯ CentOS 7 ç¦ç”¨firewalld ç¦ç”¨selinux æ—¶é’ŸåŒæ­¥ è®¾ç½®ä¸»æœºå ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. å®‰è£…ä¾èµ–åŒ… æ‰€æœ‰èŠ‚ç‚¹éƒ½éœ€è¦å®‰è£…
å®‰è£…epelå’Œelrepoè½¯ä»¶æº yum install -y epel-release elrepo-release å®‰è£…python3 yum install -y python3 å‡çº§kernelè‡³æœ€æ–° yum --enablerepo=elrepo-kernel install -y kernel-ml # è®¾ç½®é»˜è®¤ä½¿ç”¨çš„å†…æ ¸ grubby --default-index grubby --info=ALL grubby --set-default-index=0 å®‰è£…docker-ceè½¯ä»¶æº cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo å®‰è£…æœ€æ–°ç‰ˆæœ¬docker yum install -y docker-ce systemctl start docker systemctl enable docker é‡å¯ç”Ÿæ•ˆ 3. å®‰è£…cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x ."><meta property="og:type" content="article"><meta property="og:url" content="https://gnail89.github.io/posts/install-ceph-octopus-with-cephadm/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-22T17:24:19+08:00"><meta property="article:modified_time" content="2022-11-22T17:24:19+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Install Ceph Octopus With Cephadm"><meta name=twitter:description content="1. ç¯å¢ƒä¿¡æ¯ CentOS 7 ç¦ç”¨firewalld ç¦ç”¨selinux æ—¶é’ŸåŒæ­¥ è®¾ç½®ä¸»æœºå ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. å®‰è£…ä¾èµ–åŒ… æ‰€æœ‰èŠ‚ç‚¹éƒ½éœ€è¦å®‰è£…
å®‰è£…epelå’Œelrepoè½¯ä»¶æº yum install -y epel-release elrepo-release å®‰è£…python3 yum install -y python3 å‡çº§kernelè‡³æœ€æ–° yum --enablerepo=elrepo-kernel install -y kernel-ml # è®¾ç½®é»˜è®¤ä½¿ç”¨çš„å†…æ ¸ grubby --default-index grubby --info=ALL grubby --set-default-index=0 å®‰è£…docker-ceè½¯ä»¶æº cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo å®‰è£…æœ€æ–°ç‰ˆæœ¬docker yum install -y docker-ce systemctl start docker systemctl enable docker é‡å¯ç”Ÿæ•ˆ 3. å®‰è£…cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x ."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://gnail89.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Install Ceph Octopus With Cephadm","item":"https://gnail89.github.io/posts/install-ceph-octopus-with-cephadm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Install Ceph Octopus With Cephadm","name":"Install Ceph Octopus With Cephadm","description":"1. ç¯å¢ƒä¿¡æ¯ CentOS 7 ç¦ç”¨firewalld ç¦ç”¨selinux æ—¶é’ŸåŒæ­¥ è®¾ç½®ä¸»æœºå ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. å®‰è£…ä¾èµ–åŒ… æ‰€æœ‰èŠ‚ç‚¹éƒ½éœ€è¦å®‰è£…\nå®‰è£…epelå’Œelrepoè½¯ä»¶æº yum install -y epel-release elrepo-release å®‰è£…python3 yum install -y python3 å‡çº§kernelè‡³æœ€æ–° yum --enablerepo=elrepo-kernel install -y kernel-ml # è®¾ç½®é»˜è®¤ä½¿ç”¨çš„å†…æ ¸ grubby --default-index grubby --info=ALL grubby --set-default-index=0 å®‰è£…docker-ceè½¯ä»¶æº cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo å®‰è£…æœ€æ–°ç‰ˆæœ¬docker yum install -y docker-ce systemctl start docker systemctl enable docker é‡å¯ç”Ÿæ•ˆ 3. å®‰è£…cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x .","keywords":["ceph","cephadm"],"articleBody":"1. ç¯å¢ƒä¿¡æ¯ CentOS 7 ç¦ç”¨firewalld ç¦ç”¨selinux æ—¶é’ŸåŒæ­¥ è®¾ç½®ä¸»æœºå ip addr hostname 192.168.61.129 ceph-node01 192.168.61.130 ceph-node02 192.168.61.131 ceph-node03 2. å®‰è£…ä¾èµ–åŒ… æ‰€æœ‰èŠ‚ç‚¹éƒ½éœ€è¦å®‰è£…\nå®‰è£…epelå’Œelrepoè½¯ä»¶æº yum install -y epel-release elrepo-release å®‰è£…python3 yum install -y python3 å‡çº§kernelè‡³æœ€æ–° yum --enablerepo=elrepo-kernel install -y kernel-ml # è®¾ç½®é»˜è®¤ä½¿ç”¨çš„å†…æ ¸ grubby --default-index grubby --info=ALL grubby --set-default-index=0 å®‰è£…docker-ceè½¯ä»¶æº cd /etc/yum.repos.d/ curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo å®‰è£…æœ€æ–°ç‰ˆæœ¬docker yum install -y docker-ce systemctl start docker systemctl enable docker é‡å¯ç”Ÿæ•ˆ 3. å®‰è£…cephadm curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm chmod +x ./cephadm ./cephadm add-repo --release octopus #æ·»åŠ repoè½¯ä»¶æº ./cephadm install #å‡çº§æœ€æ–°cephadmåŒ… 4. åˆå§‹åŒ–cephé›†ç¾¤ åˆå§‹åŒ–ç¬¬ä¸€ä¸ªmonèŠ‚ç‚¹ cephadm install ceph-common #å®‰è£…ceph cliå·¥å…· #æŸ¥çœ‹ç¼–æ’åç«¯æ˜¯cephadmï¼Œå¦‚æœæ˜¯æ˜¾ç¤ºrookè¿™é‡Œåç«¯å°±ä¼šæ˜¾ç¤ºrookã€‚ ceph orch status mkdir -p /etc/ceph cephadm bootstrap --mon-ip 192.168.61.129 #åˆå§‹åŒ–ä¸€ä¸ªmonèŠ‚ç‚¹ï¼Œè·å–æœ€æ–°çš„å®¹å™¨é•œåƒåˆ°æœ¬åœ° # è·å–monå®¹å™¨shell cephadm shell --fsid ec45b570-5432-11ed-9ea9-000c29cb9f51 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring å¼•å¯¼å®Œæˆä¸€ä¸ªå•èŠ‚ç‚¹ç¾¤é›†ï¼Œç¨‹åºä¼šåšå¦‚ä¸‹äº‹æƒ…ï¼š\nåœ¨æœ¬åœ°ä¸»æœºä¸Šä¸ºæ–°é›†ç¾¤åˆ›å»ºmonitor å’Œ manager daemonå®ˆæŠ¤ç¨‹åºã€‚ ä¸ºCephé›†ç¾¤ç”Ÿæˆä¸€ä¸ªæ–°çš„SSHå¯†é’¥ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°rootç”¨æˆ·çš„/root/.ssh/authorized_keysæ–‡ä»¶ä¸­ã€‚ å°†ä¸æ–°ç¾¤é›†è¿›è¡Œé€šä¿¡æ‰€éœ€çš„æœ€å°é…ç½®æ–‡ä»¶ä¿å­˜åˆ°/etc/ceph/ceph.confã€‚ å‘/etc/ceph/ceph.client.admin.keyringå†™å…¥client.adminç®¡ç†secret keyçš„å‰¯æœ¬ã€‚ å°†public keyçš„å‰¯æœ¬å†™å…¥/etc/ceph/ceph.pubã€‚ åˆå§‹çš„mgrä¿¡æ¯ï¼š\nURL: https://ceph-node01:8443 User: admin Password: å®‰è£…æç¤ºè·å–å¯†ç  (å¯é€‰)å¦‚æœå¿˜è®°mgrå¯†ç å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•é‡ç½®å¯†ç (å°†å¯†ç å†™å…¥passwordæ–‡ä»¶ä¸­ï¼Œé€šè¿‡å‘½ä»¤å¯¼å…¥å¯†ç )\nceph dashboard ac-user-set-password admin -i password å½“å‰èŠ‚ç‚¹å®‰è£…åå®¹å™¨åˆ—è¡¨ä¸­çš„è§’è‰²ï¼š mon.ceph-node01 # monitorèŠ‚ç‚¹ mgr.ceph-node01.hnlomt # mgrç®¡ç†èŠ‚ç‚¹ prometheus.ceph-node01 # prometheusç›‘æ§ alertmanager.ceph-node01 # prometheuså‘Šè­¦ç›¸å…³ grafana.ceph-node01 # ç›‘æ§å‘Šè­¦å±•ç¤º node-exporter.ceph-node01 # æ•°æ®é‡‡é›†nodeèŠ‚ç‚¹ crash.ceph-node01 # crashç›¸å…³ åˆ†å‘cephadmèŠ‚ç‚¹å…¬é’¥åˆ°å…¶ä½™èŠ‚ç‚¹ï¼š ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node02 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node03 æ–°å¢cephä¸»æœºåˆ°é›†ç¾¤æ–°å¢cephä¸»æœºåˆ°é›†ç¾¤ ceph orch host add ceph-node02 192.168.61.130 ceph orch host add ceph-node03 192.168.61.131 è°ƒæ•´monèŠ‚ç‚¹åˆ†å¸ƒ é»˜è®¤monèŠ‚ç‚¹æ˜¯5ä¸ªï¼Œéšæœºåˆ†å¸ƒåœ¨é›†ç¾¤ä¸»æœºä¸Šï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µï¼ŒæŒ‡å®šéƒ¨ç½²ceph monèŠ‚ç‚¹ï¼ˆorchæ¨¡å—è‡ªåŠ¨ç®¡ç†ï¼‰\nceph orch apply mon \"ceph-node01,ceph-node02,ceph-node03\" æŒ‡å®šéƒ¨ç½²monæ•°é‡ï¼Œorchæ¨¡å—è‡ªåŠ¨éƒ¨ç½²ï¼Œé»˜è®¤å€¼ä¸º5 ceph orch apply mon 3 æŒ‡å®šéƒ¨ç½²monåˆ°æŒ‡å®šæ ‡ç­¾èŠ‚ç‚¹ï¼ˆorchæ¨¡å—è‡ªåŠ¨ç®¡ç†ï¼‰ ceph orch apply mon label:mon ç¦ç”¨monè‡ªåŠ¨éƒ¨ç½²ï¼ˆå¯é€‰ï¼‰ è¿™ç§æƒ…å†µä¸‹éœ€è¦æ‰‹åŠ¨éƒ¨ç½²monèŠ‚ç‚¹ï¼ŒmonèŠ‚ç‚¹ä¸å—orchæ¨¡å—è‡ªåŠ¨ç®¡ç†\nceph orch apply mon --unmanaged æ‰‹åŠ¨æŒ‡å®šèŠ‚ç‚¹éƒ¨ç½²monï¼ˆå¯é€‰ï¼‰ ceph orch daemon add mon ceph-node02:192.168.61.130 ceph orch daemon add mon ceph-node03:192.168.61.131 å¯¹èŠ‚ç‚¹æ‰“æ ‡ç­¾(å¯é€‰) ceph orch host label add ceph-node01 mon ceph orch host label add ceph-node02 mon ceph orch host label add ceph-node03 mon ceph orch host label add ceph-node01 _admin ceph orch host label add ceph-node01 mgr ceph orch host label add ceph-node02 mgr æŸ¥è¯¢é›†ç¾¤çŠ¶æ€ æŸ¥çœ‹ServiceçŠ¶æ€ ceph orch ls NAME RUNNING REFRESHED AGE PLACEMENT IMAGNAME IMAGE ID alertmanager 1/1 7m ago 23h count:1 quay.iprometheus/alertmanager:v0.20.0 0881eb8f169f crash 3/3 7m ago 23h * quay.io/cepceph:v15 93146564743f grafana 1/1 7m ago 23h count:1 quay.io/cepceph-grafana:6.7.4 557c83e11646 mgr 2/2 7m ago 23h count:2 quay.io/cepceph:v15 93146564743f mon 3/3 7m ago 118m ceph-node01;ceph-node02;ceph-node03 quay.io/cepceph:v15 93146564743f node-exporter 3/3 7m ago 23h * quay.iprometheus/node-exporter:v0.18.1 e5a616e4b9cf prometheus 1/1 7m ago 23h count:1 quay.iprometheus/prometheus:v2.18.1 de242295e225 æŸ¥è¯¢DeamonçŠ¶æ€ ceph orch ps NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ceph-node01 ceph-node01 running (2h) 3m ago 4w 0.20.0 quay.io/prometheus/alertmanager:v0.20.0 0881eb8f169f 689490756acc crash.ceph-node01 ceph-node01 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 5c05118fec32 crash.ceph-node02 ceph-node02 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f bbc408935136 crash.ceph-node03 ceph-node03 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 87c1e705053e grafana.ceph-node01 ceph-node01 running (2h) 3m ago 4w 6.7.4 quay.io/ceph/ceph-grafana:6.7.4 557c83e11646 e211ea6be042 mgr.ceph-node01.wmeddo ceph-node01 running (2h) 3m ago 5d 15.2.17 quay.io/ceph/ceph:v15 93146564743f e465de9c3e9d mgr.ceph-node02.hyipxw ceph-node02 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 5026b159b240 mon.ceph-node01 ceph-node01 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 3f48f4045628 mon.ceph-node02 ceph-node02 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 3cf45764cafb mon.ceph-node03 ceph-node03 running (2h) 3m ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f f689c97923f6 node-exporter.ceph-node01 ceph-node01 running (2h) 3m ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 12aec21e4c45 node-exporter.ceph-node02 ceph-node02 running (2h) 3m ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 4fcfb820aa3b node-exporter.ceph-node03 ceph-node03 running (2h) 3m ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 8aee37a4868b prometheus.ceph-node01 ceph-node01 running (2h) 3m ago 4w 2.18.1 quay.io/prometheus/prometheus:v2.18.1 de242295e225 aa30615a1ce0 æŸ¥çœ‹orchç®¡ç†çš„ä¸»æœºåˆ—è¡¨ ceph orch host ls HOST ADDR LABELS STATUS ceph-node01 ceph-node01 _admin mgr mon ceph-node02 ceph-node02 mgr mon ceph-node03 ceph-node03 mon 5. éƒ¨ç½²osd æ·»åŠ OSDéœ€æ±‚æ»¡è¶³ä»¥ä¸‹æ‰€æœ‰æ¡ä»¶ï¼š\nè®¾å¤‡å¿…é¡»æ²¡æœ‰åˆ†åŒºã€‚\nè®¾å¤‡ä¸å¾—å…·æœ‰ä»»ä½•LVMçŠ¶æ€ã€‚\nä¸å¾—å®‰è£…è®¾å¤‡ã€‚\nè¯¥è®¾å¤‡ä¸å¾—åŒ…å«æ–‡ä»¶ç³»ç»Ÿã€‚\nè¯¥è®¾å¤‡ä¸å¾—åŒ…å«Ceph BlueStore OSDã€‚\nè®¾å¤‡å¿…é¡»å¤§äº5 GBã€‚\norchæ¨¡å—è‡ªåŠ¨éƒ¨ç½²osd ceph orch apply osd --all-available-devices æš‚åœorchæ¨¡å—è‡ªåŠ¨ç®¡ç†osdï¼ˆå¯é€‰ï¼‰ ceph orch apply osd --all-available-devices --unmanaged=true æ‰‹åŠ¨æ–°å¢osdï¼ˆå¯é€‰ï¼‰ ceph orch device ls ceph orch device zap ceph-node01 /dev/sdb --force ceph orch daemon add osd ceph-node01:/dev/sdb ç§»é™¤osd ceph orch daemon stop osd.2 ceph orch osd rm 2 #ç­‰å¾…æ•°æ®å¹³è¡¡ ceph orch osd rm status ceph osd tree ceph osd out osd.2 #ç­‰å¾…æ•°æ®å¹³è¡¡ ceph osd purge osd.2 --force ceph osd crush ls ceph-node01 ceph orch daemon rm osd.2 --force é—®é¢˜ï¼šwipefs: error: /dev/sdb: probing initialization failed: Device or resource busy è§£å†³åŠæ³•ï¼š\nç§»é™¤é€»è¾‘å·ï¼š dmsetup remove --force ceph--cd03e720--2f3e--497e--8f7a--97432c2b675d-osd--block--c4b4700e--1001--473a--ab05--470b7e9c711e æ“¦é™¤æ–‡ä»¶ç³»ç»Ÿçš„ç­¾å wipefs -a /dev/sdb é‡æ–°åˆå§‹åŒ– ceph orch device zap ceph-node01 /dev/sdb --force é—®é¢˜ï¼šé›†ç¾¤æ‰€æœ‰mgrå…¨éƒ¨downï¼Œé›†ç¾¤æŠ¥warningï¼Œæ— æ³•ä½¿ç”¨orchç®¡ç†ã€‚ æ‰¾åˆ°mgrä¸€ä¸ªèŠ‚ç‚¹ï¼Œmgrä¸€èˆ¬ä¿å­˜ä½ç½®ä¸ºï¼š/var/lib/ceph/{fsid}/mgr.node.name\né€šè¿‡cephadmä¸´æ—¶æ‹‰èµ·mgræœåŠ¡ï¼š\ncephadm run --name {mgr.node.name} --fsid {fsid} ç„¶åç”¨orchæ¨¡å—æ‹‰èµ·å‰©ä½™mgræœåŠ¡ï¼š ceph orch ps ceph orch daemon start mgr.ceph-node01.name åœæ‰cephadmä¸´æ—¶æ‹‰èµ·çš„mgrï¼Œç„¶åç”¨orchæ¨¡å—é‡å¯æ‰€æœ‰mgrï¼š ceph orch restart mgr 6. éƒ¨ç½²CEPHFSæœåŠ¡ ä½¿ç”¨ceph fsåˆ›å»º åˆ›å»ºcephfsæ–‡ä»¶ç³»ç»Ÿï¼Œæ–°ç‰ˆæœ¬cephä¼šè‡ªåŠ¨åˆ›å»ºmdsæœåŠ¡ï¼Œä½¿ç”¨cephfsæ–‡ä»¶ç³»ç»Ÿéœ€è¦ä¸€ä¸ªæˆ–å¤šä¸ªMDSæœåŠ¡ã€‚\nceph fs volume create cephfs --placement=3 ceph fs status # æˆ–è€…æ‰‹åŠ¨éƒ¨ç½²mdså®ˆæŠ¤è¿›ç¨‹ï¼š ceph orch apply mds cephfs --placement 3 ceph orch ls æŸ¥çœ‹ceph fsçŠ¶æ€ï¼Œå¯ä»¥æ˜¾ç¤ºä¸»å¤‡mdsèŠ‚ç‚¹ä¿¡æ¯ï¼Œæ–‡ä»¶ç³»ç»Ÿæƒ…å†µ\nceph fs status ceph fs ls ceph mds stat æ‰‹åŠ¨åˆ›å»º # åˆ›å»ºfs dataæ± ï¼Œå¯ç”¨erasureçº åˆ åŠŸèƒ½ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨replicatedåŠŸèƒ½ã€‚ ceph osd pool create cephfs.cephfs.data 32 erasure # åˆ›å»ºfs metadataæ± ï¼Œåªèƒ½é»˜è®¤replicatedå¤åˆ¶åŠŸèƒ½ ceph osd pool create cephfs.cephfs.meta 32 # åˆ›å»ºceph fsï¼Œé™„ä¸Šmetadataå’Œdataæ±  ceph fs new cephfs cephfs.cephfs.meta cephfs.cephfs.data # æ‰‹åŠ¨éƒ¨ç½²cephfs mdsæœåŠ¡ ceph orch apply mds cephfs --placement=3 ï¼ˆå¯é€‰ï¼‰å¯å°†cephfsçš„æ•°æ®æ± è®¾ç½®å¯ç”¨çº åˆ ç (poolåˆ›å»ºæ—¶éœ€è¦æŒ‡å®šä¸ºerasureå‚æ•°)ï¼Œç„¶åå¯ç”¨ec overwritesã€‚ä½†cephfs metadataå…ƒæ•°æ®æ± ä¸èƒ½ä½¿ç”¨çº åˆ ç ï¼Œå› ä¸ºå…ƒæ•°æ®ä½¿ç”¨OMAPæ•°æ®ç»“æ„æ‰€ä»¥ä¸æ”¯æŒã€‚\nceph osd pool set cephfs.cephfs.data allow_ec_overwrites true LinuxæŒ‚è½½cephfsæ–‡ä»¶ç³»ç»Ÿæ–¹æ³• ç”Ÿæˆcephé…ç½®æ–‡ä»¶è‡³å®¢æˆ·ç«¯ä¸»æœº mkdir -p -m 755 /etc/ceph ssh {user}@{mon-host} \"sudo ceph config generate-minimal-conf\" | sudo tee /etc/ceph/ceph.conf chmod 644 /etc/ceph/ceph.conf ç”Ÿæˆcephxè®¤è¯ä¿¡æ¯ ssh {user}@{mon-host} \"sudo ceph fs authorize cephfs client.mountfs / rw\" | sudo tee /etceph/ceph.client.mountfs.keyring chmod 600 /etc/ceph/ceph.client.mountfs.keyring å½“ä½¿ç”¨å†…æ ¸é©±åŠ¨æŒ‚è½½cephfsæ—¶ä½¿ç”¨ï¼ˆæ³„éœ²å¯†é’¥é£é™©ï¼‰ # mount -t ceph {device-string}:{path-to-mounted} {mount-point} -o {key-value-args{other-args} mount -t ceph 192.168.61.129:6789,192.168.61.130:6789,192.168.61.131:6789:/ /mnt -name=mountfs,secret=AQBtO3dj8bjLNxAAGFHw02lPnlbapyAVwwIBuA== ä½¿ç”¨mount.cephæŒ‚è½½ï¼ˆéœ€è¦å®‰è£…ceph-commonåŒ…ï¼‰ mount -t ceph :/ /mnt -o name=mountfs,fs=cephfs å†™å…¥/etc/fstabæ–‡ä»¶ :/ /mnt ceph name=mountfs,[fs=cephfs,secret=str,]noatime,[nodiratime,]_netdev 0 2 7. éƒ¨ç½²CEPH NFSæœåŠ¡ ä»…æ”¯æŒ NFSv4 åè®®ã€‚\nåˆ›å»ºæ–°æ±  ceph osd pool create ganesha_data 32 ceph osd pool application enable ganesha_data nfs åˆ›å»ºNFSç½‘å…³ ceph orch apply nfs nfs ganesha_data --placement=2 æŸ¥çœ‹ServiceçŠ¶æ€ ceph orch ls NAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID alertmanager 1/1 46s ago 4w count:1 quay.io/prometheus/alertmanager:v0.20.0 0881eb8f169f crash 3/3 48s ago 4w * quay.io/ceph/ceph:v15 93146564743f grafana 1/1 46s ago 4w count:1 quay.io/ceph/ceph-grafana:6.7.4 557c83e11646 mds.cephfs 2/2 48s ago 5d count:2 quay.io/ceph/ceph:v15 93146564743f mgr 2/2 48s ago 9d ceph-node01;ceph-node02 quay.io/ceph/ceph:v15 93146564743f mon 3/3 48s ago 4w ceph-node01;ceph-node02;ceph-node03 quay.io/ceph/ceph:v15 93146564743f nfs.nfs 2/2 46s ago 11m count:2 quay.io/ceph/ceph:v15 93146564743f node-exporter 3/3 48s ago 4w * quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf osd.all-available-devices 3/3 48s ago 9d * quay.io/ceph/ceph:v15 93146564743f prometheus 1/1 46s ago 4w count:1 quay.io/prometheus/prometheus:v2.18.1 de242295e225 æŸ¥çœ‹daemonçŠ¶æ€ ceph orch ps NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ceph-node01 ceph-node01 running (5h) 81s ago 4w 0.20.0 quay.io/prometheus/alertmanager:v0.20.0 0881eb8f169f 689490756acc crash.ceph-node01 ceph-node01 running (5h) 81s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 5c05118fec32 crash.ceph-node02 ceph-node02 running (5h) 84s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f bbc408935136 crash.ceph-node03 ceph-node03 running (5h) 58s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 87c1e705053e grafana.ceph-node01 ceph-node01 running (5h) 81s ago 4w 6.7.4 quay.io/ceph/ceph-grafana:6.7.4 557c83e11646 e211ea6be042 mds.cephfs.ceph-node02.mxqelr ceph-node02 running (5h) 84s ago 5d 15.2.17 quay.io/ceph/ceph:v15 93146564743f 549606028df4 mds.cephfs.ceph-node03.xhcgto ceph-node03 running (5h) 58s ago 5d 15.2.17 quay.io/ceph/ceph:v15 93146564743f ee51fb9c1a8b mgr.ceph-node01.wmeddo ceph-node01 running (5h) 81s ago 6d 15.2.17 quay.io/ceph/ceph:v15 93146564743f e465de9c3e9d mgr.ceph-node02.hyipxw ceph-node02 running (5h) 84s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 5026b159b240 mon.ceph-node01 ceph-node01 running (5h) 81s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 3f48f4045628 mon.ceph-node02 ceph-node02 running (5h) 84s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 3cf45764cafb mon.ceph-node03 ceph-node03 running (5h) 58s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f f689c97923f6 nfs.nfs.ceph-node01 ceph-node01 running (11m) 81s ago 11m 3.3 quay.io/ceph/ceph:v15 93146564743f 117e58e5990c nfs.nfs.ceph-node03 ceph-node03 running (60s) 58s ago 11m 3.3 quay.io/ceph/ceph:v15 93146564743f 3662ecedd542 node-exporter.ceph-node01 ceph-node01 running (5h) 81s ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 12aec21e4c45 node-exporter.ceph-node02 ceph-node02 running (5h) 84s ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 4fcfb820aa3b node-exporter.ceph-node03 ceph-node03 running (5h) 58s ago 4w 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 8aee37a4868b osd.0 ceph-node03 running (5h) 58s ago 4w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 4d8635a833cc osd.1 ceph-node02 running (5h) 84s ago 3w 15.2.17 quay.io/ceph/ceph:v15 93146564743f 6fa84fe728c7 osd.2 ceph-node01 running (5h) 81s ago 9d 15.2.17 quay.io/ceph/ceph:v15 93146564743f 16d42b3583ea prometheus.ceph-node01 ceph-node01 running (5h) 81s ago 4w 2.18.1 quay.io/prometheus/prometheus:v2.18.1 de242295e225 aa30615a1ce0 NFS é«˜å¯ç”¨(é«˜ç‰ˆæœ¬æ”¯æŒ) ingress nfs éƒ¨ç½²æ–‡ä»¶ï¼šingress-nfs.yml service_type: ingress service_id: nfs.nfs placement: count: 2 spec: backend_service: nfs.nfs frontend_port: 2050 # å¯¹å¤–æœåŠ¡ç«¯å£ monitor_port: 9000 # haproxyæŸ¥çœ‹çŠ¶æ€é¡µé¢ virtual_ip: 192.168.61.140/24 éƒ¨ç½² ceph orch apply -i ingress-nfs.yaml 8. éƒ¨ç½²RGWæœåŠ¡ ceph orch apply rgw object-service us-east-1 --placement=3 é»˜è®¤æƒ…å†µä¸‹ï¼Œcephadmä¼šè‡ªåŠ¨åˆ›å»ºrealmå’Œzoneï¼Œæˆ–è€…ä¹Ÿå¯ä»¥æ‰‹åŠ¨åˆ›å»ºrealmï¼Œzonegroupï¼Œzoneã€‚\nradosgw-admin realm create --rgw-realm= --default radosgw-admin zonegroup create --rgw-zonegroup= --master --default radosgw-admin zone create --rgw-zonegroup= --rgw-zone= --master --default radosgw-admin period update --rgw-realm= --commit ","wordCount":"1244","inLanguage":"en","datePublished":"2022-11-22T17:24:19+08:00","dateModified":"2022-11-22T17:24:19+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gnail89.github.io/posts/install-ceph-octopus-with-cephadm/"},"publisher":{"@type":"Organization","name":"ğŸ“•W.'s Blog","logo":{"@type":"ImageObject","url":"https://gnail89.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gnail89.github.io accesskey=h title="ğŸ“•W.'s Blog (Alt + H)">ğŸ“•W.'s Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://gnail89.github.io/search/ title=ğŸ”Search><span>ğŸ”Search</span></a></li><li><a href=https://gnail89.github.io/archives/ title=ğŸ“šArchive><span>ğŸ“šArchive</span></a></li><li><a href=https://gnail89.github.io/tags/ title=ğŸ”–Tags><span>ğŸ”–Tags</span></a></li><li><a href=https://gnail89.github.io/categories/ title=ğŸ§©Categories><span>ğŸ§©Categories</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gnail89.github.io>Home</a>&nbsp;Â»&nbsp;<a href=https://gnail89.github.io/posts/>Posts</a></div><h1 class=post-title>Install Ceph Octopus With Cephadm</h1><div class=post-meta>&lt;span title='2022-11-22 17:24:19 +0800 +0800'>November 22, 2022&lt;/span>&amp;nbsp;Â·&amp;nbsp;6 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-%e7%8e%af%e5%a2%83%e4%bf%a1%e6%81%af aria-label="1. ç¯å¢ƒä¿¡æ¯">1. ç¯å¢ƒä¿¡æ¯</a></li><li><a href=#2-%e5%ae%89%e8%a3%85%e4%be%9d%e8%b5%96%e5%8c%85 aria-label="2. å®‰è£…ä¾èµ–åŒ…">2. å®‰è£…ä¾èµ–åŒ…</a></li><li><a href=#3-%e5%ae%89%e8%a3%85cephadm aria-label="3. å®‰è£…cephadm">3. å®‰è£…cephadm</a></li><li><a href=#4-%e5%88%9d%e5%a7%8b%e5%8c%96ceph%e9%9b%86%e7%be%a4 aria-label="4. åˆå§‹åŒ–cephé›†ç¾¤">4. åˆå§‹åŒ–cephé›†ç¾¤</a><ul><li><a href=#%e5%88%9d%e5%a7%8b%e5%8c%96%e7%ac%ac%e4%b8%80%e4%b8%aamon%e8%8a%82%e7%82%b9 aria-label=åˆå§‹åŒ–ç¬¬ä¸€ä¸ªmonèŠ‚ç‚¹>åˆå§‹åŒ–ç¬¬ä¸€ä¸ªmonèŠ‚ç‚¹</a></li><li><a href=#%e6%9f%a5%e8%af%a2%e9%9b%86%e7%be%a4%e7%8a%b6%e6%80%81 aria-label=æŸ¥è¯¢é›†ç¾¤çŠ¶æ€>æŸ¥è¯¢é›†ç¾¤çŠ¶æ€</a></li></ul></li><li><a href=#5-%e9%83%a8%e7%bd%b2osd aria-label="5. éƒ¨ç½²osd">5. éƒ¨ç½²osd</a><ul><li><a href=#%e9%97%ae%e9%a2%98wipefs-error-devsdb-probing-initialization-failed-device-or-resource-busy aria-label="é—®é¢˜ï¼šwipefs: error: /dev/sdb: probing initialization failed: Device or resource busy">é—®é¢˜ï¼šwipefs: error: /dev/sdb: probing initialization failed: Device or resource busy</a></li><li><a href=#%e9%97%ae%e9%a2%98%e9%9b%86%e7%be%a4%e6%89%80%e6%9c%89mgr%e5%85%a8%e9%83%a8down%e9%9b%86%e7%be%a4%e6%8a%a5warning%e6%97%a0%e6%b3%95%e4%bd%bf%e7%94%a8orch%e7%ae%a1%e7%90%86 aria-label=é—®é¢˜ï¼šé›†ç¾¤æ‰€æœ‰mgrå…¨éƒ¨downï¼Œé›†ç¾¤æŠ¥warningï¼Œæ— æ³•ä½¿ç”¨orchç®¡ç†ã€‚>é—®é¢˜ï¼šé›†ç¾¤æ‰€æœ‰mgrå…¨éƒ¨downï¼Œé›†ç¾¤æŠ¥warningï¼Œæ— æ³•ä½¿ç”¨orchç®¡ç†ã€‚</a></li></ul></li><li><a href=#6-%e9%83%a8%e7%bd%b2cephfs%e6%9c%8d%e5%8a%a1 aria-label="6. éƒ¨ç½²CEPHFSæœåŠ¡">6. éƒ¨ç½²CEPHFSæœåŠ¡</a><ul><li><a href=#%e4%bd%bf%e7%94%a8ceph-fs%e5%88%9b%e5%bb%ba aria-label="ä½¿ç”¨ceph fsåˆ›å»º">ä½¿ç”¨ceph fsåˆ›å»º</a></li><li><a href=#%e6%89%8b%e5%8a%a8%e5%88%9b%e5%bb%ba aria-label=æ‰‹åŠ¨åˆ›å»º>æ‰‹åŠ¨åˆ›å»º</a></li><li><a href=#linux%e6%8c%82%e8%bd%bdcephfs%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e6%96%b9%e6%b3%95 aria-label=LinuxæŒ‚è½½cephfsæ–‡ä»¶ç³»ç»Ÿæ–¹æ³•>LinuxæŒ‚è½½cephfsæ–‡ä»¶ç³»ç»Ÿæ–¹æ³•</a></li></ul></li><li><a href=#7-%e9%83%a8%e7%bd%b2ceph-nfs%e6%9c%8d%e5%8a%a1 aria-label="7. éƒ¨ç½²CEPH NFSæœåŠ¡">7. éƒ¨ç½²CEPH NFSæœåŠ¡</a><ul><li><a href=#%e5%88%9b%e5%bb%ba%e6%96%b0%e6%b1%a0 aria-label=åˆ›å»ºæ–°æ± >åˆ›å»ºæ–°æ± </a></li><li><a href=#%e5%88%9b%e5%bb%banfs%e7%bd%91%e5%85%b3 aria-label=åˆ›å»ºNFSç½‘å…³>åˆ›å»ºNFSç½‘å…³</a></li><li><a href=#nfs-%e9%ab%98%e5%8f%af%e7%94%a8%e9%ab%98%e7%89%88%e6%9c%ac%e6%94%af%e6%8c%81 aria-label="NFS é«˜å¯ç”¨(é«˜ç‰ˆæœ¬æ”¯æŒ)">NFS é«˜å¯ç”¨(é«˜ç‰ˆæœ¬æ”¯æŒ)</a></li></ul></li><li><a href=#8-%e9%83%a8%e7%bd%b2rgw%e6%9c%8d%e5%8a%a1 aria-label="8. éƒ¨ç½²RGWæœåŠ¡">8. éƒ¨ç½²RGWæœåŠ¡</a></li></ul></div></details></div><div class=post-content><h1 id=1-ç¯å¢ƒä¿¡æ¯>1. ç¯å¢ƒä¿¡æ¯<a hidden class=anchor aria-hidden=true href=#1-ç¯å¢ƒä¿¡æ¯>#</a></h1><ul><li>CentOS 7</li><li>ç¦ç”¨firewalld</li><li>ç¦ç”¨selinux</li><li>æ—¶é’ŸåŒæ­¥</li><li>è®¾ç½®ä¸»æœºå</li></ul><table><thead><tr><th>ip addr</th><th>hostname</th></tr></thead><tbody><tr><td>192.168.61.129</td><td>ceph-node01</td></tr><tr><td>192.168.61.130</td><td>ceph-node02</td></tr><tr><td>192.168.61.131</td><td>ceph-node03</td></tr></tbody></table><h1 id=2-å®‰è£…ä¾èµ–åŒ…>2. å®‰è£…ä¾èµ–åŒ…<a hidden class=anchor aria-hidden=true href=#2-å®‰è£…ä¾èµ–åŒ…>#</a></h1><blockquote><p>æ‰€æœ‰èŠ‚ç‚¹éƒ½éœ€è¦å®‰è£…</p></blockquote><ul><li>å®‰è£…epelå’Œelrepoè½¯ä»¶æº</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yum install -y epel-release elrepo-release
</span></span></code></pre></div><ul><li>å®‰è£…python3</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yum install -y python3
</span></span></code></pre></div><ul><li>å‡çº§kernelè‡³æœ€æ–°</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yum --enablerepo<span class=o>=</span>elrepo-kernel install -y kernel-ml
</span></span><span class=line><span class=cl><span class=c1># è®¾ç½®é»˜è®¤ä½¿ç”¨çš„å†…æ ¸</span>
</span></span><span class=line><span class=cl>grubby --default-index
</span></span><span class=line><span class=cl>grubby --info<span class=o>=</span>ALL
</span></span><span class=line><span class=cl>grubby --set-default-index<span class=o>=</span><span class=m>0</span>
</span></span></code></pre></div><ul><li>å®‰è£…docker-ceè½¯ä»¶æº</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>cd</span> /etc/yum.repos.d/
</span></span><span class=line><span class=cl>curl --remote-name https://download.docker.com/linux/centos/docker-ce.repo
</span></span></code></pre></div><ul><li>å®‰è£…æœ€æ–°ç‰ˆæœ¬docker</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yum install -y docker-ce
</span></span><span class=line><span class=cl>systemctl start docker
</span></span><span class=line><span class=cl>systemctl <span class=nb>enable</span> docker
</span></span></code></pre></div><ul><li>é‡å¯ç”Ÿæ•ˆ</li></ul><h1 id=3-å®‰è£…cephadm>3. å®‰è£…cephadm<a hidden class=anchor aria-hidden=true href=#3-å®‰è£…cephadm>#</a></h1><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl --remote-name --location https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-octopus/elnoarch/cephadm
</span></span><span class=line><span class=cl>chmod +x ./cephadm
</span></span><span class=line><span class=cl>./cephadm add-repo --release octopus        <span class=c1>#æ·»åŠ repoè½¯ä»¶æº</span>
</span></span><span class=line><span class=cl>./cephadm install    <span class=c1>#å‡çº§æœ€æ–°cephadmåŒ…</span>
</span></span></code></pre></div><h1 id=4-åˆå§‹åŒ–cephé›†ç¾¤>4. åˆå§‹åŒ–cephé›†ç¾¤<a hidden class=anchor aria-hidden=true href=#4-åˆå§‹åŒ–cephé›†ç¾¤>#</a></h1><h2 id=åˆå§‹åŒ–ç¬¬ä¸€ä¸ªmonèŠ‚ç‚¹>åˆå§‹åŒ–ç¬¬ä¸€ä¸ªmonèŠ‚ç‚¹<a hidden class=anchor aria-hidden=true href=#åˆå§‹åŒ–ç¬¬ä¸€ä¸ªmonèŠ‚ç‚¹>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cephadm install ceph-common     <span class=c1>#å®‰è£…ceph cliå·¥å…·</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#æŸ¥çœ‹ç¼–æ’åç«¯æ˜¯cephadmï¼Œå¦‚æœæ˜¯æ˜¾ç¤ºrookè¿™é‡Œåç«¯å°±ä¼šæ˜¾ç¤ºrookã€‚</span>
</span></span><span class=line><span class=cl>ceph orch status
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>mkdir -p /etc/ceph
</span></span><span class=line><span class=cl>cephadm bootstrap --mon-ip 192.168.61.129       <span class=c1>#åˆå§‹åŒ–ä¸€ä¸ªmonèŠ‚ç‚¹ï¼Œè·å–æœ€æ–°çš„å®¹å™¨é•œåƒåˆ°æœ¬åœ°</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># è·å–monå®¹å™¨shell</span>
</span></span><span class=line><span class=cl>cephadm shell --fsid ec45b570-5432-11ed-9ea9-000c29cb9f51 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring
</span></span></code></pre></div><p>å¼•å¯¼å®Œæˆä¸€ä¸ªå•èŠ‚ç‚¹ç¾¤é›†ï¼Œç¨‹åºä¼šåšå¦‚ä¸‹äº‹æƒ…ï¼š</p><ul><li>åœ¨æœ¬åœ°ä¸»æœºä¸Šä¸ºæ–°é›†ç¾¤åˆ›å»ºmonitor å’Œ manager daemonå®ˆæŠ¤ç¨‹åºã€‚</li><li>ä¸ºCephé›†ç¾¤ç”Ÿæˆä¸€ä¸ªæ–°çš„SSHå¯†é’¥ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°rootç”¨æˆ·çš„/root/.ssh/authorized_keysæ–‡ä»¶ä¸­ã€‚</li><li>å°†ä¸æ–°ç¾¤é›†è¿›è¡Œé€šä¿¡æ‰€éœ€çš„æœ€å°é…ç½®æ–‡ä»¶ä¿å­˜åˆ°/etc/ceph/ceph.confã€‚</li><li>å‘/etc/ceph/ceph.client.admin.keyringå†™å…¥client.adminç®¡ç†secret keyçš„å‰¯æœ¬ã€‚</li><li>å°†public keyçš„å‰¯æœ¬å†™å…¥/etc/ceph/ceph.pubã€‚</li></ul><blockquote><p>åˆå§‹çš„mgrä¿¡æ¯ï¼š</p></blockquote><pre tabindex=0><code>URL: https://ceph-node01:8443
User: admin
Password: å®‰è£…æç¤ºè·å–å¯†ç 
</code></pre><p>(å¯é€‰)å¦‚æœå¿˜è®°mgrå¯†ç å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•é‡ç½®å¯†ç (å°†å¯†ç å†™å…¥passwordæ–‡ä»¶ä¸­ï¼Œé€šè¿‡å‘½ä»¤å¯¼å…¥å¯†ç )</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph dashboard ac-user-set-password admin -i password
</span></span></code></pre></div><ul><li>å½“å‰èŠ‚ç‚¹å®‰è£…åå®¹å™¨åˆ—è¡¨ä¸­çš„è§’è‰²ï¼š</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>mon.ceph-node01             <span class=c1># monitorèŠ‚ç‚¹</span>
</span></span><span class=line><span class=cl>mgr.ceph-node01.hnlomt      <span class=c1># mgrç®¡ç†èŠ‚ç‚¹</span>
</span></span><span class=line><span class=cl>prometheus.ceph-node01      <span class=c1># prometheusç›‘æ§</span>
</span></span><span class=line><span class=cl>alertmanager.ceph-node01    <span class=c1># prometheuså‘Šè­¦ç›¸å…³</span>
</span></span><span class=line><span class=cl>grafana.ceph-node01         <span class=c1># ç›‘æ§å‘Šè­¦å±•ç¤º</span>
</span></span><span class=line><span class=cl>node-exporter.ceph-node01   <span class=c1># æ•°æ®é‡‡é›†nodeèŠ‚ç‚¹</span>
</span></span><span class=line><span class=cl>crash.ceph-node01           <span class=c1># crashç›¸å…³</span>
</span></span></code></pre></div><ul><li>åˆ†å‘cephadmèŠ‚ç‚¹å…¬é’¥åˆ°å…¶ä½™èŠ‚ç‚¹ï¼š</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node02
</span></span><span class=line><span class=cl>ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node03
</span></span></code></pre></div><ul><li>æ–°å¢cephä¸»æœºåˆ°é›†ç¾¤æ–°å¢cephä¸»æœºåˆ°é›†ç¾¤</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch host add ceph-node02 192.168.61.130
</span></span><span class=line><span class=cl>ceph orch host add ceph-node03 192.168.61.131
</span></span></code></pre></div><ul><li>è°ƒæ•´monèŠ‚ç‚¹åˆ†å¸ƒ</li></ul><p>é»˜è®¤monèŠ‚ç‚¹æ˜¯5ä¸ªï¼Œéšæœºåˆ†å¸ƒåœ¨é›†ç¾¤ä¸»æœºä¸Šï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µï¼ŒæŒ‡å®šéƒ¨ç½²ceph monèŠ‚ç‚¹ï¼ˆorchæ¨¡å—è‡ªåŠ¨ç®¡ç†ï¼‰</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply mon <span class=s2>&#34;ceph-node01,ceph-node02,ceph-node03&#34;</span>
</span></span></code></pre></div><ul><li>æŒ‡å®šéƒ¨ç½²monæ•°é‡ï¼Œorchæ¨¡å—è‡ªåŠ¨éƒ¨ç½²ï¼Œé»˜è®¤å€¼ä¸º5</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply mon <span class=m>3</span>
</span></span></code></pre></div><ul><li>æŒ‡å®šéƒ¨ç½²monåˆ°æŒ‡å®šæ ‡ç­¾èŠ‚ç‚¹ï¼ˆorchæ¨¡å—è‡ªåŠ¨ç®¡ç†ï¼‰</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply mon label:mon
</span></span></code></pre></div><ul><li>ç¦ç”¨monè‡ªåŠ¨éƒ¨ç½²ï¼ˆå¯é€‰ï¼‰</li></ul><p>è¿™ç§æƒ…å†µä¸‹éœ€è¦æ‰‹åŠ¨éƒ¨ç½²monèŠ‚ç‚¹ï¼ŒmonèŠ‚ç‚¹ä¸å—orchæ¨¡å—è‡ªåŠ¨ç®¡ç†</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply mon --unmanaged
</span></span></code></pre></div><ul><li>æ‰‹åŠ¨æŒ‡å®šèŠ‚ç‚¹éƒ¨ç½²monï¼ˆå¯é€‰ï¼‰</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch daemon add mon ceph-node02:192.168.61.130
</span></span><span class=line><span class=cl>ceph orch daemon add mon ceph-node03:192.168.61.131
</span></span></code></pre></div><ul><li>å¯¹èŠ‚ç‚¹æ‰“æ ‡ç­¾(å¯é€‰)</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch host label add ceph-node01 mon
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node02 mon
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node03 mon
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node01 _admin
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node01 mgr
</span></span><span class=line><span class=cl>ceph orch host label add ceph-node02 mgr
</span></span></code></pre></div><h2 id=æŸ¥è¯¢é›†ç¾¤çŠ¶æ€>æŸ¥è¯¢é›†ç¾¤çŠ¶æ€<a hidden class=anchor aria-hidden=true href=#æŸ¥è¯¢é›†ç¾¤çŠ¶æ€>#</a></h2><ul><li>æŸ¥çœ‹ServiceçŠ¶æ€</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ls
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>NAME           RUNNING  REFRESHED  AGE   PLACEMENT                            IMAGNAME                                IMAGE ID      
</span></span><span class=line><span class=cl>alertmanager       1/1  7m ago     23h   count:1                              quay.iprometheus/alertmanager:v0.20.0   0881eb8f169f  
</span></span><span class=line><span class=cl>crash              3/3  7m ago     23h   *                                    quay.io/cepceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>grafana            1/1  7m ago     23h   count:1                              quay.io/cepceph-grafana:6.7.4           557c83e11646  
</span></span><span class=line><span class=cl>mgr                2/2  7m ago     23h   count:2                              quay.io/cepceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>mon                3/3  7m ago     118m  ceph-node01<span class=p>;</span>ceph-node02<span class=p>;</span>ceph-node03  quay.io/cepceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>node-exporter      3/3  7m ago     23h   *                                    quay.iprometheus/node-exporter:v0.18.1  e5a616e4b9cf  
</span></span><span class=line><span class=cl>prometheus         1/1  7m ago     23h   count:1                              quay.iprometheus/prometheus:v2.18.1     de242295e225  
</span></span></code></pre></div><ul><li>æŸ¥è¯¢DeamonçŠ¶æ€</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ps
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                           HOST         STATUS        REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID  
</span></span><span class=line><span class=cl>alertmanager.ceph-node01       ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   0.20.0   quay.io/prometheus/alertmanager:v0.20.0   0881eb8f169f  689490756acc  
</span></span><span class=line><span class=cl>crash.ceph-node01              ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  5c05118fec32  
</span></span><span class=line><span class=cl>crash.ceph-node02              ceph-node02  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  bbc408935136  
</span></span><span class=line><span class=cl>crash.ceph-node03              ceph-node03  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  87c1e705053e  
</span></span><span class=line><span class=cl>grafana.ceph-node01            ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   6.7.4    quay.io/ceph/ceph-grafana:6.7.4           557c83e11646  e211ea6be042  
</span></span><span class=line><span class=cl>mgr.ceph-node01.wmeddo         ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     5d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  e465de9c3e9d  
</span></span><span class=line><span class=cl>mgr.ceph-node02.hyipxw         ceph-node02  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  5026b159b240  
</span></span><span class=line><span class=cl>mon.ceph-node01                ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  3f48f4045628  
</span></span><span class=line><span class=cl>mon.ceph-node02                ceph-node02  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  3cf45764cafb  
</span></span><span class=line><span class=cl>mon.ceph-node03                ceph-node03  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  f689c97923f6  
</span></span><span class=line><span class=cl>node-exporter.ceph-node01      ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  12aec21e4c45  
</span></span><span class=line><span class=cl>node-exporter.ceph-node02      ceph-node02  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  4fcfb820aa3b  
</span></span><span class=line><span class=cl>node-exporter.ceph-node03      ceph-node03  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  8aee37a4868b  
</span></span><span class=line><span class=cl>prometheus.ceph-node01         ceph-node01  running <span class=o>(</span>2h<span class=o>)</span>  3m ago     4w   2.18.1   quay.io/prometheus/prometheus:v2.18.1     de242295e225  aa30615a1ce0  
</span></span></code></pre></div><ul><li>æŸ¥çœ‹orchç®¡ç†çš„ä¸»æœºåˆ—è¡¨</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch host ls
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>HOST         ADDR         LABELS          STATUS  
</span></span><span class=line><span class=cl>ceph-node01  ceph-node01  _admin mgr mon          
</span></span><span class=line><span class=cl>ceph-node02  ceph-node02  mgr mon                 
</span></span><span class=line><span class=cl>ceph-node03  ceph-node03  mon 
</span></span></code></pre></div><h1 id=5-éƒ¨ç½²osd>5. éƒ¨ç½²osd<a hidden class=anchor aria-hidden=true href=#5-éƒ¨ç½²osd>#</a></h1><p>æ·»åŠ OSDéœ€æ±‚æ»¡è¶³ä»¥ä¸‹æ‰€æœ‰æ¡ä»¶ï¼š</p><blockquote><p>è®¾å¤‡å¿…é¡»æ²¡æœ‰åˆ†åŒºã€‚<br>è®¾å¤‡ä¸å¾—å…·æœ‰ä»»ä½•LVMçŠ¶æ€ã€‚<br>ä¸å¾—å®‰è£…è®¾å¤‡ã€‚<br>è¯¥è®¾å¤‡ä¸å¾—åŒ…å«æ–‡ä»¶ç³»ç»Ÿã€‚<br>è¯¥è®¾å¤‡ä¸å¾—åŒ…å«Ceph BlueStore OSDã€‚<br>è®¾å¤‡å¿…é¡»å¤§äº5 GBã€‚</p></blockquote><ul><li>orchæ¨¡å—è‡ªåŠ¨éƒ¨ç½²osd</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply osd --all-available-devices
</span></span></code></pre></div><ul><li>æš‚åœorchæ¨¡å—è‡ªåŠ¨ç®¡ç†osdï¼ˆå¯é€‰ï¼‰</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply osd --all-available-devices --unmanaged<span class=o>=</span><span class=nb>true</span>
</span></span></code></pre></div><ul><li>æ‰‹åŠ¨æ–°å¢osdï¼ˆå¯é€‰ï¼‰</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch device ls
</span></span><span class=line><span class=cl>ceph orch device zap ceph-node01 /dev/sdb --force
</span></span><span class=line><span class=cl>ceph orch daemon add osd ceph-node01:/dev/sdb
</span></span></code></pre></div><ul><li>ç§»é™¤osd</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch daemon stop osd.2
</span></span><span class=line><span class=cl>ceph orch osd rm <span class=m>2</span>  <span class=c1>#ç­‰å¾…æ•°æ®å¹³è¡¡</span>
</span></span><span class=line><span class=cl>ceph orch osd rm status
</span></span><span class=line><span class=cl>ceph osd tree
</span></span><span class=line><span class=cl>ceph osd out osd.2      <span class=c1>#ç­‰å¾…æ•°æ®å¹³è¡¡</span>
</span></span><span class=line><span class=cl>ceph osd purge osd.2 --force
</span></span><span class=line><span class=cl>ceph osd crush ls ceph-node01
</span></span><span class=line><span class=cl>ceph orch daemon rm osd.2 --force
</span></span></code></pre></div><h2 id=é—®é¢˜wipefs-error-devsdb-probing-initialization-failed-device-or-resource-busy>é—®é¢˜ï¼šwipefs: error: /dev/sdb: probing initialization failed: Device or resource busy<a hidden class=anchor aria-hidden=true href=#é—®é¢˜wipefs-error-devsdb-probing-initialization-failed-device-or-resource-busy>#</a></h2><p>è§£å†³åŠæ³•ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ç§»é™¤é€»è¾‘å·ï¼š
</span></span><span class=line><span class=cl>dmsetup remove --force ceph--cd03e720--2f3e--497e--8f7a--97432c2b675d-osd--block--c4b4700e--1001--473a--ab05--470b7e9c711e
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>æ“¦é™¤æ–‡ä»¶ç³»ç»Ÿçš„ç­¾å
</span></span><span class=line><span class=cl>wipefs -a /dev/sdb
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>é‡æ–°åˆå§‹åŒ–
</span></span><span class=line><span class=cl>ceph orch device zap ceph-node01 /dev/sdb --force
</span></span></code></pre></div><h2 id=é—®é¢˜é›†ç¾¤æ‰€æœ‰mgrå…¨éƒ¨downé›†ç¾¤æŠ¥warningæ— æ³•ä½¿ç”¨orchç®¡ç†>é—®é¢˜ï¼šé›†ç¾¤æ‰€æœ‰mgrå…¨éƒ¨downï¼Œé›†ç¾¤æŠ¥warningï¼Œæ— æ³•ä½¿ç”¨orchç®¡ç†ã€‚<a hidden class=anchor aria-hidden=true href=#é—®é¢˜é›†ç¾¤æ‰€æœ‰mgrå…¨éƒ¨downé›†ç¾¤æŠ¥warningæ— æ³•ä½¿ç”¨orchç®¡ç†>#</a></h2><ul><li><p>æ‰¾åˆ°mgrä¸€ä¸ªèŠ‚ç‚¹ï¼Œmgrä¸€èˆ¬ä¿å­˜ä½ç½®ä¸ºï¼š<code>/var/lib/ceph/{fsid}/mgr.node.name</code></p></li><li><p>é€šè¿‡cephadmä¸´æ—¶æ‹‰èµ·mgræœåŠ¡ï¼š</p></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cephadm run --name <span class=o>{</span>mgr.node.name<span class=o>}</span> --fsid <span class=o>{</span>fsid<span class=o>}</span>
</span></span></code></pre></div><ul><li>ç„¶åç”¨orchæ¨¡å—æ‹‰èµ·å‰©ä½™mgræœåŠ¡ï¼š</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ps
</span></span><span class=line><span class=cl>ceph orch daemon start mgr.ceph-node01.name
</span></span></code></pre></div><ul><li>åœæ‰cephadmä¸´æ—¶æ‹‰èµ·çš„mgrï¼Œç„¶åç”¨orchæ¨¡å—é‡å¯æ‰€æœ‰mgrï¼š</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch restart mgr
</span></span></code></pre></div><h1 id=6-éƒ¨ç½²cephfsæœåŠ¡>6. éƒ¨ç½²CEPHFSæœåŠ¡<a hidden class=anchor aria-hidden=true href=#6-éƒ¨ç½²cephfsæœåŠ¡>#</a></h1><h2 id=ä½¿ç”¨ceph-fsåˆ›å»º>ä½¿ç”¨ceph fsåˆ›å»º<a hidden class=anchor aria-hidden=true href=#ä½¿ç”¨ceph-fsåˆ›å»º>#</a></h2><p>åˆ›å»ºcephfsæ–‡ä»¶ç³»ç»Ÿï¼Œæ–°ç‰ˆæœ¬cephä¼šè‡ªåŠ¨åˆ›å»ºmdsæœåŠ¡ï¼Œä½¿ç”¨cephfsæ–‡ä»¶ç³»ç»Ÿéœ€è¦ä¸€ä¸ªæˆ–å¤šä¸ªMDSæœåŠ¡ã€‚</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph fs volume create cephfs --placement<span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl>ceph fs status
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># æˆ–è€…æ‰‹åŠ¨éƒ¨ç½²mdså®ˆæŠ¤è¿›ç¨‹ï¼š</span>
</span></span><span class=line><span class=cl>ceph orch apply mds cephfs --placement <span class=m>3</span>
</span></span><span class=line><span class=cl>ceph orch ls
</span></span></code></pre></div><p>æŸ¥çœ‹ceph fsçŠ¶æ€ï¼Œå¯ä»¥æ˜¾ç¤ºä¸»å¤‡mdsèŠ‚ç‚¹ä¿¡æ¯ï¼Œæ–‡ä»¶ç³»ç»Ÿæƒ…å†µ</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph fs status
</span></span><span class=line><span class=cl>ceph fs ls
</span></span><span class=line><span class=cl>ceph mds stat
</span></span></code></pre></div><h2 id=æ‰‹åŠ¨åˆ›å»º>æ‰‹åŠ¨åˆ›å»º<a hidden class=anchor aria-hidden=true href=#æ‰‹åŠ¨åˆ›å»º>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># åˆ›å»ºfs dataæ± ï¼Œå¯ç”¨erasureçº åˆ åŠŸèƒ½ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨replicatedåŠŸèƒ½ã€‚</span>
</span></span><span class=line><span class=cl>ceph osd pool create cephfs.cephfs.data <span class=m>32</span> erasure
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># åˆ›å»ºfs metadataæ± ï¼Œåªèƒ½é»˜è®¤replicatedå¤åˆ¶åŠŸèƒ½</span>
</span></span><span class=line><span class=cl>ceph osd pool create cephfs.cephfs.meta <span class=m>32</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># åˆ›å»ºceph fsï¼Œé™„ä¸Šmetadataå’Œdataæ± </span>
</span></span><span class=line><span class=cl>ceph fs new cephfs cephfs.cephfs.meta cephfs.cephfs.data
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># æ‰‹åŠ¨éƒ¨ç½²cephfs mdsæœåŠ¡</span>
</span></span><span class=line><span class=cl>ceph orch apply mds cephfs --placement<span class=o>=</span><span class=m>3</span>
</span></span></code></pre></div><p>ï¼ˆå¯é€‰ï¼‰å¯å°†cephfsçš„æ•°æ®æ± è®¾ç½®å¯ç”¨çº åˆ ç (poolåˆ›å»ºæ—¶éœ€è¦æŒ‡å®šä¸ºerasureå‚æ•°)ï¼Œç„¶åå¯ç”¨ec overwritesã€‚ä½†cephfs metadataå…ƒæ•°æ®æ± ä¸èƒ½ä½¿ç”¨çº åˆ ç ï¼Œå› ä¸ºå…ƒæ•°æ®ä½¿ç”¨OMAPæ•°æ®ç»“æ„æ‰€ä»¥ä¸æ”¯æŒã€‚</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph osd pool <span class=nb>set</span> cephfs.cephfs.data allow_ec_overwrites <span class=nb>true</span>
</span></span></code></pre></div><h2 id=linuxæŒ‚è½½cephfsæ–‡ä»¶ç³»ç»Ÿæ–¹æ³•>LinuxæŒ‚è½½cephfsæ–‡ä»¶ç³»ç»Ÿæ–¹æ³•<a hidden class=anchor aria-hidden=true href=#linuxæŒ‚è½½cephfsæ–‡ä»¶ç³»ç»Ÿæ–¹æ³•>#</a></h2><ul><li>ç”Ÿæˆcephé…ç½®æ–‡ä»¶è‡³å®¢æˆ·ç«¯ä¸»æœº</li></ul><pre tabindex=0><code>mkdir -p -m 755 /etc/ceph
ssh {user}@{mon-host} &#34;sudo ceph config generate-minimal-conf&#34; | sudo tee /etc/ceph/ceph.conf
chmod 644 /etc/ceph/ceph.conf
</code></pre><ul><li>ç”Ÿæˆcephxè®¤è¯ä¿¡æ¯</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh <span class=o>{</span>user<span class=o>}</span>@<span class=o>{</span>mon-host<span class=o>}</span> <span class=s2>&#34;sudo ceph fs authorize cephfs client.mountfs / rw&#34;</span> <span class=p>|</span> sudo tee /etceph/ceph.client.mountfs.keyring
</span></span><span class=line><span class=cl>chmod <span class=m>600</span> /etc/ceph/ceph.client.mountfs.keyring
</span></span></code></pre></div><ul><li>å½“ä½¿ç”¨å†…æ ¸é©±åŠ¨æŒ‚è½½cephfsæ—¶ä½¿ç”¨ï¼ˆæ³„éœ²å¯†é’¥é£é™©ï¼‰</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># mount -t ceph {device-string}:{path-to-mounted} {mount-point} -o {key-value-args{other-args}</span>
</span></span><span class=line><span class=cl>mount -t ceph 192.168.61.129:6789,192.168.61.130:6789,192.168.61.131:6789:/ /mnt -name<span class=o>=</span>mountfs,secret<span class=o>=</span><span class=nv>AQBtO3dj8bjLNxAAGFHw02lPnlbapyAVwwIBuA</span><span class=o>==</span>
</span></span></code></pre></div><ul><li>ä½¿ç”¨mount.cephæŒ‚è½½ï¼ˆéœ€è¦å®‰è£…ceph-commonåŒ…ï¼‰</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>mount -t ceph :/ /mnt -o <span class=nv>name</span><span class=o>=</span>mountfs,fs<span class=o>=</span>cephfs
</span></span></code></pre></div><ul><li>å†™å…¥/etc/fstabæ–‡ä»¶</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>:/  /mnt  ceph  <span class=nv>name</span><span class=o>=</span>mountfs,<span class=o>[</span><span class=nv>fs</span><span class=o>=</span>cephfs,secret<span class=o>=</span>str,<span class=o>]</span>noatime,<span class=o>[</span>nodiratime,<span class=o>]</span>_netdev  <span class=m>0</span>  <span class=m>2</span>
</span></span></code></pre></div><h1 id=7-éƒ¨ç½²ceph-nfsæœåŠ¡>7. éƒ¨ç½²CEPH NFSæœåŠ¡<a hidden class=anchor aria-hidden=true href=#7-éƒ¨ç½²ceph-nfsæœåŠ¡>#</a></h1><blockquote><p>ä»…æ”¯æŒ NFSv4 åè®®ã€‚</p></blockquote><h2 id=åˆ›å»ºæ–°æ± >åˆ›å»ºæ–°æ± <a hidden class=anchor aria-hidden=true href=#åˆ›å»ºæ–°æ± >#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph osd pool create ganesha_data <span class=m>32</span>
</span></span><span class=line><span class=cl>ceph osd pool application <span class=nb>enable</span> ganesha_data nfs
</span></span></code></pre></div><h2 id=åˆ›å»ºnfsç½‘å…³>åˆ›å»ºNFSç½‘å…³<a hidden class=anchor aria-hidden=true href=#åˆ›å»ºnfsç½‘å…³>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply nfs nfs ganesha_data --placement<span class=o>=</span><span class=m>2</span> 
</span></span></code></pre></div><ul><li>æŸ¥çœ‹ServiceçŠ¶æ€</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ls
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                       RUNNING  REFRESHED  AGE  PLACEMENT                            IMAGE NAME                                IMAGE ID      
</span></span><span class=line><span class=cl>alertmanager                   1/1  46s ago    4w   count:1                              quay.io/prometheus/alertmanager:v0.20.0   0881eb8f169f  
</span></span><span class=line><span class=cl>crash                          3/3  48s ago    4w   *                                    quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>grafana                        1/1  46s ago    4w   count:1                              quay.io/ceph/ceph-grafana:6.7.4           557c83e11646  
</span></span><span class=line><span class=cl>mds.cephfs                     2/2  48s ago    5d   count:2                              quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>mgr                            2/2  48s ago    9d   ceph-node01<span class=p>;</span>ceph-node02              quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>mon                            3/3  48s ago    4w   ceph-node01<span class=p>;</span>ceph-node02<span class=p>;</span>ceph-node03  quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>nfs.nfs                        2/2  46s ago    11m  count:2                              quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>node-exporter                  3/3  48s ago    4w   *                                    quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  
</span></span><span class=line><span class=cl>osd.all-available-devices      3/3  48s ago    9d   *                                    quay.io/ceph/ceph:v15                     93146564743f  
</span></span><span class=line><span class=cl>prometheus                     1/1  46s ago    4w   count:1                              quay.io/prometheus/prometheus:v2.18.1     de242295e225  
</span></span></code></pre></div><ul><li>æŸ¥çœ‹daemonçŠ¶æ€</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ps
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                           HOST         STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID  
</span></span><span class=line><span class=cl>alertmanager.ceph-node01       ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   0.20.0   quay.io/prometheus/alertmanager:v0.20.0   0881eb8f169f  689490756acc  
</span></span><span class=line><span class=cl>crash.ceph-node01              ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  5c05118fec32  
</span></span><span class=line><span class=cl>crash.ceph-node02              ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  bbc408935136  
</span></span><span class=line><span class=cl>crash.ceph-node03              ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  87c1e705053e  
</span></span><span class=line><span class=cl>grafana.ceph-node01            ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   6.7.4    quay.io/ceph/ceph-grafana:6.7.4           557c83e11646  e211ea6be042  
</span></span><span class=line><span class=cl>mds.cephfs.ceph-node02.mxqelr  ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    5d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  549606028df4  
</span></span><span class=line><span class=cl>mds.cephfs.ceph-node03.xhcgto  ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    5d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  ee51fb9c1a8b  
</span></span><span class=line><span class=cl>mgr.ceph-node01.wmeddo         ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    6d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  e465de9c3e9d  
</span></span><span class=line><span class=cl>mgr.ceph-node02.hyipxw         ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  5026b159b240  
</span></span><span class=line><span class=cl>mon.ceph-node01                ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  3f48f4045628  
</span></span><span class=line><span class=cl>mon.ceph-node02                ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  3cf45764cafb  
</span></span><span class=line><span class=cl>mon.ceph-node03                ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  f689c97923f6  
</span></span><span class=line><span class=cl>nfs.nfs.ceph-node01            ceph-node01  running <span class=o>(</span>11m<span class=o>)</span>  81s ago    11m  3.3      quay.io/ceph/ceph:v15                     93146564743f  117e58e5990c  
</span></span><span class=line><span class=cl>nfs.nfs.ceph-node03            ceph-node03  running <span class=o>(</span>60s<span class=o>)</span>  58s ago    11m  3.3      quay.io/ceph/ceph:v15                     93146564743f  3662ecedd542  
</span></span><span class=line><span class=cl>node-exporter.ceph-node01      ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  12aec21e4c45  
</span></span><span class=line><span class=cl>node-exporter.ceph-node02      ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  4fcfb820aa3b  
</span></span><span class=line><span class=cl>node-exporter.ceph-node03      ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    4w   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  8aee37a4868b  
</span></span><span class=line><span class=cl>osd.0                          ceph-node03  running <span class=o>(</span>5h<span class=o>)</span>   58s ago    4w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  4d8635a833cc  
</span></span><span class=line><span class=cl>osd.1                          ceph-node02  running <span class=o>(</span>5h<span class=o>)</span>   84s ago    3w   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  6fa84fe728c7  
</span></span><span class=line><span class=cl>osd.2                          ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    9d   15.2.17  quay.io/ceph/ceph:v15                     93146564743f  16d42b3583ea  
</span></span><span class=line><span class=cl>prometheus.ceph-node01         ceph-node01  running <span class=o>(</span>5h<span class=o>)</span>   81s ago    4w   2.18.1   quay.io/prometheus/prometheus:v2.18.1     de242295e225  aa30615a1ce0  
</span></span></code></pre></div><h2 id=nfs-é«˜å¯ç”¨é«˜ç‰ˆæœ¬æ”¯æŒ>NFS é«˜å¯ç”¨(é«˜ç‰ˆæœ¬æ”¯æŒ)<a hidden class=anchor aria-hidden=true href=#nfs-é«˜å¯ç”¨é«˜ç‰ˆæœ¬æ”¯æŒ>#</a></h2><ul><li>ingress nfs éƒ¨ç½²æ–‡ä»¶ï¼š<code>ingress-nfs.yml</code></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>service_type</span><span class=p>:</span><span class=w> </span><span class=l>ingress</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>service_id</span><span class=p>:</span><span class=w> </span><span class=l>nfs.nfs</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>placement</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>count</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>backend_service</span><span class=p>:</span><span class=w> </span><span class=l>nfs.nfs</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>frontend_port</span><span class=p>:</span><span class=w> </span><span class=m>2050</span><span class=w>   </span><span class=c># å¯¹å¤–æœåŠ¡ç«¯å£</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>monitor_port</span><span class=p>:</span><span class=w> </span><span class=m>9000</span><span class=w>    </span><span class=c># haproxyæŸ¥çœ‹çŠ¶æ€é¡µé¢</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>virtual_ip</span><span class=p>:</span><span class=w> </span><span class=m>192.168.61.140</span><span class=l>/24</span><span class=w>
</span></span></span></code></pre></div><ul><li>éƒ¨ç½²</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply -i ingress-nfs.yaml
</span></span></code></pre></div><h1 id=8-éƒ¨ç½²rgwæœåŠ¡>8. éƒ¨ç½²RGWæœåŠ¡<a hidden class=anchor aria-hidden=true href=#8-éƒ¨ç½²rgwæœåŠ¡>#</a></h1><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch apply rgw object-service us-east-1 --placement<span class=o>=</span><span class=m>3</span>
</span></span></code></pre></div><p>é»˜è®¤æƒ…å†µä¸‹ï¼Œcephadmä¼šè‡ªåŠ¨åˆ›å»ºrealmå’Œzoneï¼Œæˆ–è€…ä¹Ÿå¯ä»¥æ‰‹åŠ¨åˆ›å»ºrealmï¼Œzonegroupï¼Œzoneã€‚</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>radosgw-admin realm create --rgw-realm<span class=o>=</span>&lt;realm-name&gt; --default
</span></span><span class=line><span class=cl>radosgw-admin zonegroup create --rgw-zonegroup<span class=o>=</span>&lt;zonegroup-name&gt;  --master --default
</span></span><span class=line><span class=cl>radosgw-admin zone create --rgw-zonegroup<span class=o>=</span>&lt;zonegroup-name&gt; --rgw-zone<span class=o>=</span>&lt;zone-name&gt; --master --default
</span></span><span class=line><span class=cl>radosgw-admin period update --rgw-realm<span class=o>=</span>&lt;realm-name&gt; --commit
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://gnail89.github.io/tags/ceph/>ceph</a></li><li><a href=https://gnail89.github.io/tags/cephadm/>cephadm</a></li></ul><nav class=paginav><a class=prev href=https://gnail89.github.io/posts/ssh-security/><span class=title>Â« Prev Page</span><br><span>SSH Security</span>
</a><a class=next href=https://gnail89.github.io/posts/markdown/><span class=title>Next Page Â»</span><br><span>MarkdownåŸºæœ¬è¯­æ³•</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://gnail89.github.io>ğŸ“•W.'s Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>